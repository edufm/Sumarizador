{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarizador de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs do usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[*NLTK corpus, \"sample_CNN\", \"CNN\"]\n",
    "corpus = \"sample_CNN\"\n",
    "\n",
    "min_sent_size = 5\n",
    "use_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus as nltk_corpus\n",
    "stopwords = nltk_corpus.stopwords.words(\"english\")\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abre o Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_conversions = {\"nt\":\"not\", \"ll\":\"will\", \"m\":\"am\", \"s\":\"TRASH\"}\n",
    "if use_stopwords:\n",
    "    manual_conversions.update({stopword:\"TRASH\" for stopword in stopwords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = re.sub(\"-LRB-|-RRB-\", \"\", sent, flags=re.DOTALL|re.MULTILINE)\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r\"[^a-z0-9\\ ]\", \"\", sent, flags=re.DOTALL|re.MULTILINE)\n",
    "    sent = re.sub(r\"[0-9]+\", \"num\", sent, flags=re.DOTALL|re.MULTILINE)\n",
    "    sent = re.sub(r\" +(?= )\", \"\", sent, flags=re.DOTALL|re.MULTILINE).strip()\n",
    "    sent = sent.split(\" \")\n",
    "    sent = [manual_conversions[word] if word in manual_conversions.keys() else word for word in sent]\n",
    "    return [word for word in sent if word != \"TRASH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if corpus != \"CNN\" and corpus != \"sample_CNN\":\n",
    "\n",
    "    if corpus == \"reuters\":\n",
    "        data = nltk_corpus.reuters\n",
    "    elif corpus == \"brown\":\n",
    "        data = nltk_corpus.brown\n",
    "    else:\n",
    "        raise ValueError(\"Corpus not implemented yet\")\n",
    "    \n",
    "    file_ids = data.fileids()\n",
    "\n",
    "    sents = {}\n",
    "    orig_text = {}\n",
    "    for storie in file_ids:\n",
    "        file_sents = data.raw(storie).split(\".\\n\") \n",
    "        orig_text[storie] = file_sents\n",
    "        preprocessed_sent = [preprocess(sent) for sent in file_sents]\n",
    "        sents[storie] = [sent for sent in preprocessed_sent if len(sent) > min_sent_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if corpus == \"sample_CNN\":\n",
    "    sents = {}\n",
    "    orig_text = {}\n",
    "    highlights = {}\n",
    "    available_stories = os.listdir(\"./cnn_stories_sample\")\n",
    "    for storie in available_stories:\n",
    "        with open(f\"./cnn_stories_sample/{storie}\", \"r\", encoding=\"utf-8\") as file:\n",
    "            file = file.read()\n",
    "        original_sents = re.sub(r\"\\n\\n\", \" \", file.split(\"@highlight\")[0], flags=re.DOTALL|re.MULTILINE).split(\" . \")\n",
    "        highlights_sents = re.sub(r\"\\n\\n*\", \" . \", \"\".join(file.split(\"@highlight\")[1:]), flags=re.DOTALL|re.MULTILINE).split(\" . \")\n",
    "\n",
    "        # Guarda o texto original e os hghlights\n",
    "        orig_text[storie] = original_sents\n",
    "        highlights[storie] = highlights_sents\n",
    "        \n",
    "        # Guarda as sentenças préprocessadas\n",
    "        preprocessed_sent = [preprocess(original_sent) for original_sent in original_sents]\n",
    "        sents[storie] = [sent for sent in preprocessed_sent if len(sent) > min_sent_size]\n",
    "\n",
    "if corpus == \"CNN\":\n",
    "    sents = {}\n",
    "    orig_text = {}\n",
    "    highlights = {}\n",
    "    available_stories = os.listdir(\"./cnn_stories_tokenized\")\n",
    "    for storie in available_stories:\n",
    "        with open(f\"./cnn_stories_sample/{storie}\", \"r\", encoding=\"utf-8\") as file:\n",
    "            file = file.read()\n",
    "        original_sents = re.sub(r\"\\n\\n\", \" \", file.split(\"@highlight\")[0], flags=re.DOTALL|re.MULTILINE).split(\" . \")\n",
    "        highlights_sents = re.sub(r\"\\n\\n*\", \" . \", \"\".join(file.split(\"@highlight\")[1:]), flags=re.DOTALL|re.MULTILINE).split(\" . \")\n",
    "\n",
    "        # Guarda o texto original e os hghlights\n",
    "        orig_text[storie] = original_sents\n",
    "        highlight[storie] = highlights_sents\n",
    "        \n",
    "        # Guarda as sentenças préprocessadas\n",
    "        preprocessed_sent = [preprocess(original_sent) for original_sent in original_sents]\n",
    "        sents[storie] = [sent for sent in preprocessed_sent if len(sent) > min_sent_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"storage/orig_sents.json\", \"w\") as file:\n",
    "    json.dump(orig_text, file)\n",
    "\n",
    "if \"CNN\" in corpus:\n",
    "    with open(\"storage/highlights.json\", \"w\") as file:\n",
    "        json.dump(sents, file)\n",
    "\n",
    "sents_reference = {}\n",
    "i = 0\n",
    "with open('storage/all_sents.txt', 'w', encoding='utf8') as file:\n",
    "    \n",
    "    for text_id, text in sents.items():\n",
    "        sents_reference[text_id] = []\n",
    "        \n",
    "        for sentence in text:\n",
    "            file.write(\" \".join([tok for tok in sentence]) + \"\\n\")\n",
    "            sents_reference[text_id].append(i)\n",
    "            i += 1\n",
    "            \n",
    "with open(\"storage/sents_reference.json\", \"w\") as file:\n",
    "    json.dump(sents_reference, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Funções relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cnn=False):\n",
    "     \n",
    "    with open('storage/all_sents.txt', 'r', encoding='utf8') as file:\n",
    "        all_sents = [sent.split(\" \") for sent in file.read().split(\"\\n\")]\n",
    "    \n",
    "    with open(\"storage/sents_reference.json\", \"r\") as file:\n",
    "        sents_reference = json.load(file)\n",
    "        \n",
    "    with open(\"storage/orig_sents.json\", \"r\") as file:\n",
    "        orig_sents = json.load(file)\n",
    "    \n",
    "    if cnn:\n",
    "        with open(\"storage/highlights.json\", \"r\") as file:\n",
    "            highlights = json.load(file)\n",
    "    else:\n",
    "        highlights = None\n",
    "        \n",
    "    return all_sents, sents_reference, orig_sents, highlights\n",
    "\n",
    "\n",
    "def find_most_relevant_cl(vecs, sents_reference, clusters=3, per_cluster_sent=1):\n",
    "    \n",
    "    best = {}\n",
    "    for text_id in sents_reference.keys():\n",
    "    \n",
    "        # Pega os vetores da sentença desse texto\n",
    "        target_vecs = vecs[sents_reference[text_id][0]:sents_reference[text_id][-1]+1] \n",
    "\n",
    "        # Faz a clusterização dessas sentenças\n",
    "        kmeans_cbow = MiniBatchKMeans(n_clusters=3, random_state=42)\n",
    "        result = kmeans_cbow.fit_transform(target_vecs)\n",
    "        df = pd.DataFrame(result)\n",
    "\n",
    "        # Seleciona a sentença mais próxima de cada centro de cluster\n",
    "        this_best = []\n",
    "        for cluster_number in range(result.shape[1]):\n",
    "            this_best.append(df[kmeans_cbow.labels_ == cluster_number].sort_values(by=cluster_number).index.values[:per_cluster_sent])\n",
    "        best[text_id] = sorted(list(np.array(this_best).flatten()))\n",
    "            \n",
    "    return best\n",
    "\n",
    "def find_most_relevant_pr(vecs, sents_reference, n_sents=3):\n",
    "\n",
    "    best = {}\n",
    "    for text_id in sents_reference.keys():\n",
    "\n",
    "        # Pega os vetores da sentença desse texto\n",
    "        target_vecs = vecs[sents_reference[text_id][0]:sents_reference[text_id][-1]+1] \n",
    "        target_vecs = [vec.toarray() for vec in target_vecs]\n",
    "        # Faz a clusterização dessas sentenças\n",
    "        sim_mat = np.zeros((len(sents_reference[text_id]), len(sents_reference[text_id])))\n",
    "        for i, v1 in enumerate(target_vecs):\n",
    "            for j, v2 in enumerate(target_vecs):\n",
    "                norm1 = np.linalg.norm(v1)\n",
    "                norm2 = np.linalg.norm(v2)\n",
    "                # Verifica se alguem vetor possui apenas zeros\n",
    "                if v1.sum() != 0 and v2.sum() != 0:\n",
    "                    # Verifica se o valor da normalização é razoavel\n",
    "                    if norm1 > np.finfo(float).eps and norm2 > np.finfo(float).eps:\n",
    "                        sim_mat[i][j] = (v1 * v2).sum() / (norm1 + norm2)\n",
    "                    else:\n",
    "                        sim_mat[i][j] = (v1 * v2).sum()\n",
    "\n",
    "        graph = nx.from_numpy_array(sim_mat)\n",
    "        pr = nx.pagerank(graph, max_iter=100)\n",
    "\n",
    "        best[text_id] = sorted(pr, key=pr.get)[:n_sents]\n",
    "        \n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data((\"CNN\" in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = TfidfVectorizer(min_df=5, \n",
    "                        max_df=0.9, \n",
    "                        max_features=5000, \n",
    "                        sublinear_tf=False, \n",
    "                        analyzer=lambda x: x)\n",
    "\n",
    "tfidf_vecs = model.fit_transform(all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cl_best = find_most_relevant_cl(tfidf_vecs, sents_reference)\n",
    "tfidf_cl_summary = {}\n",
    "for text_id in tfidf_cl_best.keys():\n",
    "    tfidf_cl_summary[text_id] = [orig_text[text_id][sent] for sent in tfidf_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pr_best = find_most_relevant_pr(tfidf_vecs, sents_reference)\n",
    "tfidf_pr_summary = {}\n",
    "for text_id in tfidf_pr_best.keys():\n",
    "    tfidf_pr_summary[text_id] = [orig_text[text_id][sent] for sent in tfidf_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(tfidf_cl_summary.keys())[np.random.randint(0, len(tfidf_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the wake of criticism over a two-game suspension for Baltimore Ravens running back Ray Rice , the NFL has established a six-game unpaid ban for personnel who violate the league 's policy on domestic violence , Commissioner Roger Goodell said Thursday. A second incident would be punished by a lifetime ban from the league , Goodell said in a letter and memo to the owners of the league 's 32 teams. Without referring to Rice by name , he acknowledged in his letter that he made the wrong decision in that case. `` I did n't get it right. Simply put , we have to do better. And we will , '' he wrote. The policy , which is effective immediately , also applies to other types of violence. `` Violations of the Personal Conduct Policy regarding assault , battery , domestic violence or sexual assault that involve physical force will be subject to a suspension without pay of six games for a first offense , with consideration given to mitigating factors , as well as a longer suspension when circumstances warrant. '' Goodell said the circumstances that would warrant a longer suspension include incidents that predate a person 's time with an NFL team or acts that involve choking , repeated blows or a weapon. They also include violence against a pregnant woman or in view of child. Lifetime bans may be appealed after a year. The players union issued a response. `` We were informed today of the NFL 's decision to increase penalties on domestic violence offenders under the Personal Conduct Policy for all NFL employees. As we do in all disciplinary matters , if we believe that players ' due process rights are infringed upon during the course of discipline , we will assert and defend our members ' rights , '' the NFL Players Association said in its statement. Read the letter and memo Rice was suspended for two games after video showed him dragging his unconscious fiancee -- whom he later married -- from an elevator. Rice resolved the charges stemming from the incident with his now-wife , Janay , and entered a pretrial intervention program in May , the NFL said. Under the program , he wo n't be prosecuted , and the charges will be expunged after a year , the league said. The punishments , both from the NFL and from the criminal justice system , were widely decried as too light , and the issue quickly spiraled into debates over domestic violence and victim-blaming. Opinion : NFL , apologize to women for Ray Rice Goodell 's letter tells teams to distribute a memo about the new policy to each player and all members of the organizations. The NFL 's Personal Conduct Policy states that employees are `` held to a higher standard '' of conduct. `` Persons who fail to live up to this standard of conduct are guilty of conduct detrimental and subject to discipline , even where the conduct itself does not result in conviction of a crime , '' the policy states. The NFL regular season is 16 games. Ravens ' Ray Rice : ` My actions were inexcusable ' \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The policy , which is effective immediately , also applies to other types of violence',\n",
       " \"As we do in all disciplinary matters , if we believe that players ' due process rights are infringed upon during the course of discipline , we will assert and defend our members ' rights , '' the NFL Players Association said in its statement\",\n",
       " 'Read the letter and memo Rice was suspended for two games after video showed him dragging his unconscious fiancee -- whom he later married -- from an elevator']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The policy , which is effective immediately , also applies to other types of violence',\n",
       " \"'' Goodell said the circumstances that would warrant a longer suspension include incidents that predate a person 's time with an NFL team or acts that involve choking , repeated blows or a weapon\",\n",
       " 'The players union issued a response']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_pr_summary[text_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word_to_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data((\"CNN\" in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_cbow = gensim.models.Word2Vec(\n",
    "    corpus_file='storage/all_sents.txt',\n",
    "    window=5,\n",
    "    size=200,\n",
    "    seed=42,\n",
    "    iter=100,\n",
    "    workers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_word_vecs(model, sent):\n",
    "    vec = np.zeros(model.wv.vector_size)\n",
    "    for word in sent:\n",
    "        if word in model:\n",
    "            vec += model.wv.get_vector(word)\n",
    "            \n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > np.finfo(float).eps:\n",
    "        vec /= norm\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vecs = scipy.sparse.csr.csr_matrix([sum_word_vecs(model_cbow, sent) for sent in all_sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cl_best = find_most_relevant_cl(word2vec_vecs, sents_reference)\n",
    "word2vec_cl_summary = {}\n",
    "for text_id in word2vec_cl_best.keys():\n",
    "    word2vec_cl_summary[text_id] = [orig_text[text_id][sent] for sent in word2vec_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "PowerIterationFailedConvergence",
     "evalue": "(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-f7f33c099b33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2vec_pr_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_most_relevant_pr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_vecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msents_reference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mword2vec_pr_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2vec_pr_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mword2vec_pr_summary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0morig_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword2vec_pr_best\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-4648de871b4d>\u001b[0m in \u001b[0;36mfind_most_relevant_pr\u001b[1;34m(vecs, sents_reference, n_sents)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_sents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\ProgramData\\Anaconda3\\envs\\NLP\\lib\\site-packages\\decorator.py:decorator-gen-434>\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\NLP\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[1;34m(not_implement_for_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetworkXNotImplemented\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnot_implement_for_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\NLP\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPowerIterationFailedConvergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')"
     ]
    }
   ],
   "source": [
    "word2vec_pr_best = find_most_relevant_pr(word2vec_vecs, sents_reference)\n",
    "word2vec_pr_summary = {}\n",
    "for text_id in word2vec_pr_best.keys():\n",
    "    word2vec_pr_summary[text_id] = [orig_text[text_id][sent] for sent in word2vec_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(word2vec_cl_summary.keys())[np.random.randint(0, len(word2vec_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-LRB- CNN -RRB- -- War-plagued Somalia , with its crumbling government infrastructure , is the world 's most corrupt country , according to a global survey by the international watchdog Transparency International. The group 's annual Corruption Perception Index measures perceived levels of public sector corruption. As was the case last year , the 2009 survey found that countries that scored lowest all have something in common : they are fragile , unstable and scarred by war or long-standing conflicts. The group scored 180 countries on a scale of 0 -LRB- perceived to be highly corrupt -RRB- to 10 -LRB- perceived to have low levels of corruption -RRB-. Somalia scored 1.1. Next came Afghanistan at 1.3 , Myanmar at 1.4 , and Sudan and Iraq -- both at 1.5 On the other end of the scale , New Zealand ranked highest at 9.4 , followed by Denmark -LRB- 9.3 -RRB- , Singapore and Sweden -LRB- 9.2 -RRB- and Switzerland -LRB- 9.0 -RRB-. The United States came it at 19 -LRB- 7.5 -RRB- and the United Kingdom was at 17 -LRB- 7.7 -RRB-. `` When essential institutions are weak or non-existent , corruption spirals out of control and the plundering of public resources feeds insecurity and impunity , '' the group said. On the other hand , countries that fared well in the survey have oversight to stem corruption. These include a well-performing judiciary , an independent media , and vigorous law enforcement , it said. \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The group 's annual Corruption Perception Index measures perceived levels of public sector corruption\",\n",
       " 'The group scored 180 countries on a scale of 0 -LRB- perceived to be highly corrupt -RRB- to 10 -LRB- perceived to have low levels of corruption -RRB-',\n",
       " 'Somalia scored 1.1']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_pr_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a881f50be461>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2vec_pr_summary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word2vec_pr_summary' is not defined"
     ]
    }
   ],
   "source": [
    "word2vec_pr_summary[text_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data((\"CNN\" in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(all_sents)\n",
    "doc2bow = [dictionary.doc2bow(sent) for sent in all_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_TOPICS = 20\n",
    "ldamodel = LdaMulticore(doc2bow, num_topics=NUM_TOPICS, id2word=dictionary, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso se queira explorar a LDA mudar para True\n",
    "if False:\n",
    "    lda_display = pyLDAvis.gensim.prepare(ldamodel, doc2bow, dictionary, sort_topics=False)\n",
    "    pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vecs = [ldamodel.get_document_topics(text) for text in doc2bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vecs = []\n",
    "for vec in raw_vecs:\n",
    "    this_vec = []\n",
    "    curr = 0\n",
    "    for i in range(NUM_TOPICS):\n",
    "        if (i == vec[curr][0]):\n",
    "            this_vec.append(vec[curr][1])\n",
    "            curr+=1\n",
    "            if curr == len(vec):\n",
    "                curr = -1\n",
    "        else:\n",
    "            this_vec.append(0)\n",
    "    lda_vecs.append(this_vec)\n",
    "    \n",
    "lda_vecs = scipy.sparse.csr.csr_matrix(lda_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cl_best = find_most_relevant_cl(lda_vecs, sents_reference)\n",
    "lda_cl_summary = {}\n",
    "for text_id in lda_cl_best.keys():\n",
    "    lda_cl_summary[text_id] = [orig_text[text_id][sent] for sent in lda_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pr_best = find_most_relevant_pr(lda_vecs, sents_reference)\n",
    "lda_pr_summary = {}\n",
    "for text_id in lda_pr_best.keys():\n",
    "    lda_pr_summary[text_id] = [orig_text[text_id][sent] for sent in lda_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(lda_cl_summary.keys())[np.random.randint(0, len(lda_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ATHENS , Georgia -LRB- CNN -RRB- -- Over the railroad tracks , near Agriculture Drive on the University of Georgia campus , sits a unique machine that may hold one of the solutions to big environmental problems like energy , food production and even global climate change. Biochar 's high carbon content and porous nature can help soil retain water , nutrients , protect soil microbes. `` This machine right here is our baby , '' said UGA research engineer Brian Bibens , who is one of a handful of researchers around the world working on alternative ways to recycle carbon. Bibens ' specialty is `` biochar , '' a highly porous charcoal made from organic waste. The raw material can be any forest , agricultural or animal waste. Some examples are woodchips , corn husks , peanut shells , even chicken manure. Bibens feeds the waste -- called `` biomass '' -- into an octagonally shaped metal barrel where it is cooked under intense heat , sometimes above 1,000 degrees Fahrenheit , the organic matter is cooked through a thermochemical process called `` pyrolysis ''. In a few hours , organic trash is transformed into charcoal-like pellets farmers can turn into fertilizer. Gasses given off during the process can be harnesed to fuel vehicles of power electric generators. Watch how biochar is made and why it 's important '' Biochar is considered by many scientists to be the `` black gold '' for agriculture. Its high carbon content and porous nature can help soil retain water , nutrients , protect soil microbes and ultimately increase crop yields while acting as natural carbon sink - sequestering CO2 and locking it into the ground. Biochar helps clean the air two ways : by preventing rotting biomass from releasing harmful CO2 into the atmosphere , and by allowing plants to safely store CO2 they pull out of the air during photosynthesis. See more about how biochar works '' `` Soil acts as an enormous carbon pool , increasing this carbon pool could significantly contribute to the reduction of CO2 in the atmosphere , '' said Christoph Steiner , one of the leading research scientist studying biochar. `` It gives us a chance to produce carbon negative energy. '' Worldwide use of biochar could cut CO2 levels by 8 parts per million within 50 years , according to NASA scientist James Hansen. Global carbon levels in the air have been steadily increasing at an alarming rate since the 1980s , according to NOAA. Since 2000 , increases of 2 parts per million of CO2 have been common , according to NOAA. During the 1980s rates increased by 1.5 ppm per year. The process of making biochar can also lead to other valuable products. Some of the gases given off during the process can be converted to electricity , others can be condensed and converted to gasoline , and there are also some pharmaceutical applications for the by-products , said Danny Day President and CEO of Eprida , a private firm in Athens , Georgia currently exploring industry applications for the biochar process. Although scientists look to biochar to improve the future , its origin lies in the past. For centuries indigenous South Americans living in the Amazon Basin used a combination of charred animal waste and wood to make `` terra preta , '' which means black earth , in Portuguese. Thousands of years later , the terra preta soil remains fertile without need for any added fertilizer , experts say. `` These terra preta soils are older than 500 years and they are still black soil and very rich in carbon , '' said Steiner , a professor at the University of Georgia. Reducing the need for deforestation to create more cropland. By using biochar concepts , terra preta soils have been proven to remain fertile for thousands of years , preventing further harmful deforestation for agricultural purposes. But still more large-scale tests need to be conducted before biochar technology can be rolled out on a global scale. Day says biomass -- that otherwise would be thrown away -- could be developed into entirely new markets for biofuels , electricity , biomass extracts and pharmaceutical applications , in addition to biochar. `` We have 3 billion people out there who are at risk for climate change and they can be making money solving our global problem , '' said Day. Industries can now begin to look at farmers around the world and pay them for their agricultural wastes , said Day. `` They can become the new affluent. '' \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Bibens ' specialty is `` biochar , '' a highly porous charcoal made from organic waste\",\n",
       " \"Bibens feeds the waste -- called `` biomass '' -- into an octagonally shaped metal barrel where it is cooked under intense heat , sometimes above 1,000 degrees Fahrenheit , the organic matter is cooked through a thermochemical process called `` pyrolysis ''\",\n",
       " 'Day says biomass -- that otherwise would be thrown away -- could be developed into entirely new markets for biofuels , electricity , biomass extracts and pharmaceutical applications , in addition to biochar']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In a few hours , organic trash is transformed into charcoal-like pellets farmers can turn into fertilizer',\n",
       " \"`` We have 3 billion people out there who are at risk for climate change and they can be making money solving our global problem , '' said Day\",\n",
       " 'Some examples are woodchips , corn husks , peanut shells , even chicken manure']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_pr_summary[text_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
