{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess(sent):\n",
    "    def convert(word):\n",
    "        # Verifica se é um número.\n",
    "        try:\n",
    "            _ = float(word)\n",
    "            return '<num>'\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Verifica se é uma palavra.\n",
    "        if word.isalpha():\n",
    "            return word.lower()\n",
    "        \n",
    "        # Caso contrário, é pontuação ou estranho.\n",
    "        return '<weird>'\n",
    "    \n",
    "    return [convert(word) for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import reuters\n",
    "# sents = [preprocess(sent) for sent in reuters.sents()]\n",
    "\n",
    "from nltk.corpus import brown\n",
    "sents = [preprocess(sent) for sent in brown.sents()]\n",
    "\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# sents = [preprocess(item.strip().split()) for item in fetch_20newsgroups()['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weird>        172869\n",
       "the             69971\n",
       "of              36412\n",
       "and             28853\n",
       "to              26158\n",
       "                ...  \n",
       "oviform             1\n",
       "saigon              1\n",
       "superlunary         1\n",
       "sublunary           1\n",
       "stupefying          1\n",
       "Name: fdist, Length: 40234, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "words = chain.from_iterable(sents)\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "ser = pd.Series(fdist, name='fdist').sort_values(ascending=False)\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<weird>',\n",
       " 'the',\n",
       " 'of',\n",
       " 'and',\n",
       " 'to',\n",
       " 'a',\n",
       " 'in',\n",
       " 'that',\n",
       " 'is',\n",
       " 'was',\n",
       " 'he',\n",
       " 'for',\n",
       " 'it',\n",
       " 'with',\n",
       " 'as',\n",
       " 'his',\n",
       " 'on',\n",
       " '<num>',\n",
       " 'be',\n",
       " 'at']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_stopwords = 20\n",
    "stopwords = list(ser.iloc[:num_stopwords].index)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model = TfidfVectorizer(min_df=5, max_df=0.9, max_features=5000, stop_words=stopwords, sublinear_tf=False, analyzer=lambda x: x)\n",
    "\n",
    "vecs = model.fit_transform(sents)\n",
    "\n",
    "words = model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<num>',\n",
       " 'a',\n",
       " 'abandoned',\n",
       " 'abel',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorbed',\n",
       " 'abstract',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accompanied',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accordance',\n",
       " 'according']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=10, n_init=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 393 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = kmeans.fit_transform(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57340, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "\n",
      "Cluster 0: [ 7258 23434 28352 28337 28325 28290 28283  7689 54144   654]\n",
      "the def a tambourine \n",
      "--------------------------------------------------------------------------------\n",
      "a knowledgeable celebrity\n",
      "--------------------------------------------------------------------------------\n",
      "a \n",
      "--------------------------------------------------------------------------------\n",
      " a \n",
      "--------------------------------------------------------------------------------\n",
      " a \n",
      "--------------------------------------------------------------------------------\n",
      " a \n",
      "--------------------------------------------------------------------------------\n",
      " a \n",
      "--------------------------------------------------------------------------------\n",
      " a horror \n",
      "--------------------------------------------------------------------------------\n",
      "quint smothered a yawn \n",
      "--------------------------------------------------------------------------------\n",
      " a missionary \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 1: [20883 10964 39325 15378  2451 46465  2475 49450 26198 21803]\n",
      "the frenchman was astonished \n",
      "--------------------------------------------------------------------------------\n",
      "the businessman was raymond thornburg \n",
      "--------------------------------------------------------------------------------\n",
      "the log was spinning \n",
      "--------------------------------------------------------------------------------\n",
      "secrecy was paramount \n",
      "--------------------------------------------------------------------------------\n",
      " giorgio was uninjured \n",
      "--------------------------------------------------------------------------------\n",
      "was \n",
      "--------------------------------------------------------------------------------\n",
      " vernava was uninjured \n",
      "--------------------------------------------------------------------------------\n",
      "it was the barkeep \n",
      "--------------------------------------------------------------------------------\n",
      "it was the influenza pandemic of \n",
      "--------------------------------------------------------------------------------\n",
      " and was repealed \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 2: [ 3945 25071 31869 36680 44826 45022 45784 46546 40070  7213]\n",
      "the boldness of champions\n",
      "--------------------------------------------------------------------------------\n",
      "filigreed perfume boxes exuded the aromas of araby \n",
      "--------------------------------------------------------------------------------\n",
      "purification of the conjugates\n",
      "--------------------------------------------------------------------------------\n",
      "the closest scrutiny is owed to the kennings and the homeric epithets \n",
      "--------------------------------------------------------------------------------\n",
      "the inspector declined \n",
      "--------------------------------------------------------------------------------\n",
      " the lieut  \n",
      "--------------------------------------------------------------------------------\n",
      "the eyelids fluttered \n",
      "--------------------------------------------------------------------------------\n",
      "the gapt \n",
      "--------------------------------------------------------------------------------\n",
      "the schoolboy \n",
      "--------------------------------------------------------------------------------\n",
      "the unoriginals\n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 3: [54107 49308 27595 46676 27600 50017 32141 27605  5513 12088]\n",
      "quint glared \n",
      "--------------------------------------------------------------------------------\n",
      " nate  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " hesperus \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " goodbye \n",
      "--------------------------------------------------------------------------------\n",
      "corollary \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " dad  \n",
      "--------------------------------------------------------------------------------\n",
      "monel tar byrd   \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 4: [   27 28324 28332 28336 28350 28359 28378 28382 28386 28390]\n",
      " <num> \n",
      "--------------------------------------------------------------------------------\n",
      " <num> \n",
      "--------------------------------------------------------------------------------\n",
      " <num> \n",
      "--------------------------------------------------------------------------------\n",
      " <num> \n",
      "--------------------------------------------------------------------------------\n",
      "<num> \n",
      "--------------------------------------------------------------------------------\n",
      "<num> \n",
      "--------------------------------------------------------------------------------\n",
      "<num> \n",
      "--------------------------------------------------------------------------------\n",
      "<num> \n",
      "--------------------------------------------------------------------------------\n",
      "<num> \n",
      "--------------------------------------------------------------------------------\n",
      "<num> \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 5: [41537 55401 54616 16663 25257 20530 57250 14740  8364  8363]\n",
      "it could have been infant for it had not survived the bassinet \n",
      "--------------------------------------------------------------------------------\n",
      " you have to ram him \n",
      "--------------------------------------------------------------------------------\n",
      " like you to have a dilatation and curettage \n",
      "--------------------------------------------------------------------------------\n",
      "if you are not well acquainted with the area in which you wish to locate or if you are not sure that you and your family will like and make a success of farming usually you would do better to rent a place for a year or two before you buy \n",
      "--------------------------------------------------------------------------------\n",
      "the anonymous correspondent did not resort to innuendoes \n",
      "--------------------------------------------------------------------------------\n",
      "the aborigine is not deceived \n",
      "--------------------------------------------------------------------------------\n",
      "you have unwittingly set in motion forces so malign so vindictive that it would be downright inhumane of me not to warn you about them \n",
      "--------------------------------------------------------------------------------\n",
      " not \n",
      "--------------------------------------------------------------------------------\n",
      " not plump \n",
      "--------------------------------------------------------------------------------\n",
      " not chubby \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 6: [51778 54836 48926 50188 29099 41802 54432 49904 28276 40785]\n",
      " i  \n",
      "--------------------------------------------------------------------------------\n",
      "i grunted sipping \n",
      "--------------------------------------------------------------------------------\n",
      "i  \n",
      "--------------------------------------------------------------------------------\n",
      "i exclaimed \n",
      "--------------------------------------------------------------------------------\n",
      "i dissent \n",
      "--------------------------------------------------------------------------------\n",
      " i  \n",
      "--------------------------------------------------------------------------------\n",
      "thankful i nadine \n",
      "--------------------------------------------------------------------------------\n",
      " i forgot  \n",
      "--------------------------------------------------------------------------------\n",
      " i \n",
      "--------------------------------------------------------------------------------\n",
      "i stiffened \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 7: [23017  8975   983  6090  6046  8606 45806 48169 47101 54923]\n",
      "the stockholder delivers his proxy \n",
      "--------------------------------------------------------------------------------\n",
      "the armchair traveler preserves his illusions \n",
      "--------------------------------------------------------------------------------\n",
      " his miracles\n",
      "--------------------------------------------------------------------------------\n",
      "his bifocals blur \n",
      "--------------------------------------------------------------------------------\n",
      "he senses his disapproval \n",
      "--------------------------------------------------------------------------------\n",
      "his red face his coarse gestures and his lustful stares bespeak his sensuality \n",
      "--------------------------------------------------------------------------------\n",
      "he was sharpening his razor \n",
      "--------------------------------------------------------------------------------\n",
      "baldness was attacking his pate \n",
      "--------------------------------------------------------------------------------\n",
      " was sterilizing his knives \n",
      "--------------------------------------------------------------------------------\n",
      "he had hobbled downstairs into the parlor in his agony and was sitting wrapped in his bathrobe on a footstool \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 8: [ 3886 54398 56049 55046 39342 49957 45249 51239 51745 43173]\n",
      "he bogeyed the \n",
      "--------------------------------------------------------------------------------\n",
      "he insulted ken thom  \n",
      "--------------------------------------------------------------------------------\n",
      "he shivered \n",
      "--------------------------------------------------------------------------------\n",
      "he shrugged noncommittally \n",
      "--------------------------------------------------------------------------------\n",
      "he asks \n",
      "--------------------------------------------------------------------------------\n",
      " he  \n",
      "--------------------------------------------------------------------------------\n",
      "he \n",
      "--------------------------------------------------------------------------------\n",
      "he ceased weeping \n",
      "--------------------------------------------------------------------------------\n",
      "he mused \n",
      "--------------------------------------------------------------------------------\n",
      "he stir \n",
      "--------------------------------------------------------------------------------\n",
      "################################################################################\n",
      "\n",
      "Cluster 9: [ 1591 23401 15881 41043 50162 34481 29336 19163 11212 38442]\n",
      "birgit nilsson will be starred \n",
      "--------------------------------------------------------------------------------\n",
      "he has his own system of shorthand devised by abbreviations  humility will be humly  with will be w  and that will be tt \n",
      "--------------------------------------------------------------------------------\n",
      " be discouraged \n",
      "--------------------------------------------------------------------------------\n",
      "to be passive to be girlishly shy was palpably absurd \n",
      "--------------------------------------------------------------------------------\n",
      "there will be romance and flirtation \n",
      "--------------------------------------------------------------------------------\n",
      "he will not be fooled by technicalities \n",
      "--------------------------------------------------------------------------------\n",
      "how will it be financed \n",
      "--------------------------------------------------------------------------------\n",
      "it would doubtless be greatly surprised to be told that in failing to be ecumenical it is really failing to be the church of christ \n",
      "--------------------------------------------------------------------------------\n",
      "pansies have to be coddled \n",
      "--------------------------------------------------------------------------------\n",
      "it will be good for you \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_sents = 10\n",
    "for cluster_number in range(result.shape[1]):\n",
    "    best_sents = (\n",
    "        df[kmeans.labels_ == cluster_number]\n",
    "        .sort_values(by=cluster_number)\n",
    "        .index\n",
    "        .values[:num_sents]\n",
    "    )\n",
    "    print('#' * 80)\n",
    "    print(f'\\nCluster {cluster_number}: {best_sents}')\n",
    "    for k in best_sents:\n",
    "        print(' '\n",
    "              .join(sents[k])\n",
    "              .replace('<weird>', '')\n",
    "              .replace('  ', ' ')\n",
    "             )\n",
    "        print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    16206\n",
       "2    10858\n",
       "0     6665\n",
       "1     4052\n",
       "8     3878\n",
       "9     3861\n",
       "5     3786\n",
       "7     3679\n",
       "6     2968\n",
       "4     1387\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(kmeans.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences.txt', 'w', encoding='utf8') as file:\n",
    "    for sentence in sents:\n",
    "        file.write(f'{\" \".join(sentence)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_cbow = gensim.models.Word2Vec(\n",
    "    corpus_file='sentences.txt',\n",
    "    window=5,\n",
    "    size=200,\n",
    "    seed=42,\n",
    "    iter=100,\n",
    "    workers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow(model, sent):\n",
    "    vec = np.zeros(model.wv.vector_size)\n",
    "    for word in sent:\n",
    "        if word in model:\n",
    "            vec += model.wv.get_vector(word)\n",
    "            \n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > np.finfo(float).eps:\n",
    "        vec /= norm\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_cbow = [cbow(model_cbow, sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cbow = MiniBatchKMeans(n_clusters=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = kmeans_cbow.fit_transform(vecs_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sents = 10\n",
    "for cluster_number in range(result.shape[1]):\n",
    "    best_sents = (\n",
    "        df[kmeans_cbow.labels_ == cluster_number]\n",
    "        .sort_values(by=cluster_number)\n",
    "        .index\n",
    "        .values[:num_sents]\n",
    "    )\n",
    "    print('#' * 80)\n",
    "    print(f'\\nCluster {cluster_number}: {best_sents}')\n",
    "    for k in best_sents:\n",
    "        print(' '\n",
    "              .join(sents[k])\n",
    "              .replace('<weird>', '')\n",
    "              .replace('  ', ' ')\n",
    "              .replace('<num>', '#')\n",
    "             )\n",
    "        print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(kmeans_cbow.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
