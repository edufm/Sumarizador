{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumarizador de texto\n",
    "\n",
    "Esse projeto tem como objetivo criar resumos para diversos textos selecionando sentenças ou palavras que melhor representem os textos pelo grupamento de seus embedings. Isso signifia que, para fazer o resumo em sentenças, cada sentença do document passará por um algoritimo de embeding que a transformará em um vetor; com todas as sentenças como vetores é possivel utilizar clustering ou pagerank, para através da similaridade das sentenças, encontrar algumas que passem a ideia geral do texto resumido.\n",
    "\n",
    "Esse tipo de sumarização é conhecido como sumarização extrativa e diversos mecanismos de embeding podem ser utilizados para traçar a seelhança entre as sentenças e ajudar no processo de extração, o principio por trás desse tipo de sumarização é que se duas sentenças são muito parecidas, você só precisa deuma delas. Alguns dos algoritimos que serão aborados nesse projeto sao: TF-IDF, CBow, Doc2Vec, LDA e Word2Vec.\n",
    "\n",
    "De forma a verificar a qualidade dos sumarios, se optou por utilizar a metrica Rouge, que calcula pelo numero de palavras em comum entre um sumario referencia e o sumario criado o F1 score, a precisão e o recall do summario. Sendo que quanto maior a precisão, maior o recall e maior o f1, melhor é o sumario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summario\n",
    "\n",
    "### [Abre o Corpus](#Open)\n",
    "\n",
    "### [Funções Relevantes](#Func)\n",
    "\n",
    "### [Sumariza em sentenças](#Sent)\n",
    "\n",
    "[TF-IDF](#tfidf)\n",
    "\n",
    "[CBow](#cbow)\n",
    "\n",
    "[Doc 2 Vec](#doc2vec)\n",
    "\n",
    "[LDA](#lda)\n",
    "\n",
    "### [Sumariza em palavras](#Word)\n",
    "\n",
    "[Word 2 Vec](#word2vec)\n",
    "\n",
    "### [Conclusão](#Conc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs do usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dados que serão estudados [\"cnn_stories_sample\", \"cnn_stories\"]\n",
    "corpus = \"cnn_stories_sample\"\n",
    "\n",
    "# Configurações para os filtros das palavras\n",
    "min_sent_size = 5\n",
    "use_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports para tratamento de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Imports para tratamento de texto\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "# Imports para algoritimos de vectorização\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Imports para algoritimos de ranqueamento\n",
    "import networkx as nx\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Imports para verificação do sumario\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "# Outros\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abre o Corpus\n",
    "<a id='Open'></a>\n",
    "\n",
    "Nessa seção o corpus selecionado será aberto, filtrado pela função preprocess e salvo em jasons e TXTs para evitar novas filtragens no futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario de palavras que devem ser removidas/modificadas\n",
    "manual_conversions = {\"nt\":\"not\", \"ll\":\"will\", \"m\":\"am\", \"s\":\"TRASH\"}\n",
    "if use_stopwords:\n",
    "    manual_conversions.update({stopword:\"TRASH\" for stopword in stopwords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que filtra as palavras de uma sentença do texto\n",
    "def preprocess(sent):\n",
    "    sent = re.sub(\"-LRB-|-RRB-\", \"\", sent, flags=re.DOTALL|re.MULTILINE)\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r\"[^a-z0-9\\ ]\", \"\", sent, flags=re.DOTALL|re.MULTILINE)\n",
    "    sent = re.sub(r\"[0-9]+\", \"num\", sent, flags=re.DOTALL|re.MULTILINE)\n",
    "    sent = re.sub(r\" +(?= )\", \"\", sent, flags=re.DOTALL|re.MULTILINE).strip()\n",
    "    sent = sent.split(\" \")\n",
    "    sent = [manual_conversions[word] if word in manual_conversions.keys() else word for word in sent]\n",
    "    return [word for word in sent if word != \"TRASH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre os dados do corpus selecionado e faz a filtragem\n",
    "sents = {}\n",
    "orig_text = {}\n",
    "highlights = {}\n",
    "available_stories = os.listdir(f\"./{corpus}\")\n",
    "for storie in available_stories:\n",
    "    with open(f\"./cnn_stories_sample/{storie}\", \"r\", encoding=\"utf-8\") as file:\n",
    "        file = file.read()\n",
    "    original_sents = re.sub(r\"\\n\\n\", \" \", file.split(\"@highlight\")[0], flags=re.DOTALL|re.MULTILINE).split(\" . \")\n",
    "    highlights_sents = re.sub(r\"\\n\\n*\", \" . \", \"\".join(file.split(\"@highlight\")[1:]), flags=re.DOTALL|re.MULTILINE)\n",
    "\n",
    "    # Guarda o texto original e os highlights\n",
    "    orig_text[storie] = original_sents\n",
    "    highlights[storie] = highlights_sents\n",
    "\n",
    "    # Guarda as sentenças préprocessadas\n",
    "    preprocessed_sent = [preprocess(original_sent) for original_sent in original_sents]\n",
    "    sents[storie] = [sent for sent in preprocessed_sent if len(sent) > min_sent_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dadods filtrados tokenizados e originais\n",
    "with open(\"storage/orig_sents.json\", \"w\") as file:\n",
    "    json.dump(orig_text, file)\n",
    "\n",
    "with open(\"storage/highlights.json\", \"w\") as file:\n",
    "    json.dump(highlights, file)\n",
    "\n",
    "sents_reference = {}\n",
    "i = 0\n",
    "with open('storage/all_sents.txt', 'w', encoding='utf8') as file:\n",
    "    \n",
    "    for text_id, text in sents.items():\n",
    "        sents_reference[text_id] = []\n",
    "        \n",
    "        for sentence in text:\n",
    "            file.write(\" \".join([tok for tok in sentence]) + \"\\n\")\n",
    "            sents_reference[text_id].append(i)\n",
    "            i += 1\n",
    "            \n",
    "with open(\"storage/sents_reference.json\", \"w\") as file:\n",
    "    json.dump(sents_reference, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Funções relevantes\n",
    "<a id='Func'></a>\n",
    "\n",
    "Nessa seção, algumas funções são criadas para evitar repetição de código em cada modelo de embeding um veez que as etapas de carregamento dos dados, clustering, pagerank, calculo de score rouge e salvamento dos sumarios é muito semelhante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "     \n",
    "    with open('storage/all_sents.txt', 'r', encoding='utf8') as file:\n",
    "        all_sents = [sent.split(\" \") for sent in file.read().split(\"\\n\")]\n",
    "    \n",
    "    with open(\"storage/sents_reference.json\", \"r\") as file:\n",
    "        sents_reference = json.load(file)\n",
    "        \n",
    "    with open(\"storage/orig_sents.json\", \"r\") as file:\n",
    "        orig_sents = json.load(file)    \n",
    "\n",
    "    with open(\"storage/highlights.json\", \"r\") as file:\n",
    "        highlights = json.load(file)\n",
    "        \n",
    "    return all_sents, sents_reference, orig_sents, highlights\n",
    "\n",
    "\n",
    "def find_most_relevant_cl(vecs, sents_reference, clusters=3, per_cluster_sent=1):\n",
    "    \n",
    "    best = {}\n",
    "    warned = False\n",
    "    for text_id in sents_reference.keys():\n",
    "    \n",
    "        # Pega os vetores da sentença desse texto\n",
    "        target_vecs = vecs[sents_reference[text_id][0]:sents_reference[text_id][-1]+1] \n",
    "\n",
    "        # Faz a clusterização dessas sentenças\n",
    "        kmeans_cbow = MiniBatchKMeans(n_clusters=3, random_state=42)\n",
    "        result = kmeans_cbow.fit_transform(target_vecs)\n",
    "        df = pd.DataFrame(result)\n",
    "\n",
    "        # Seleciona a sentença mais próxima de cada centro de cluster\n",
    "        this_best = []\n",
    "        for cluster_number in range(result.shape[1]):\n",
    "            result = df[kmeans_cbow.labels_ == cluster_number].sort_values(by=cluster_number).index.values[:per_cluster_sent]\n",
    "            if len(result) > 0:\n",
    "                \n",
    "                result = list(result)\n",
    "                while len(result) < per_cluster_sent:\n",
    "                    result.append(0)\n",
    "                    \n",
    "                this_best.append(result)\n",
    "                \n",
    "            elif not warned:\n",
    "                warned = True\n",
    "                warnings.warn(\"No center vector found in one of the clusters\", RuntimeWarning)\n",
    "                \n",
    "        best[text_id] = set(sorted(list(np.array(this_best).flatten())))\n",
    "            \n",
    "    return best\n",
    "\n",
    "def find_most_relevant_pr(vecs, sents_reference, n_sents=3, squared=False):\n",
    "\n",
    "    best = {}\n",
    "    for text_id in sents_reference.keys():\n",
    "\n",
    "        # Pega os vetores da sentença desse texto\n",
    "        target_vecs = vecs[sents_reference[text_id][0]:sents_reference[text_id][-1]+1] \n",
    "        target_vecs = [vec.toarray() for vec in target_vecs]\n",
    "        # Faz a clusterização dessas sentenças\n",
    "        sim_mat = np.zeros((len(sents_reference[text_id]), len(sents_reference[text_id])))\n",
    "        for i, v1 in enumerate(target_vecs):\n",
    "            for j, v2 in enumerate(target_vecs):\n",
    "                norm1 = np.linalg.norm(v1)\n",
    "                norm2 = np.linalg.norm(v2)\n",
    "                # Verifica se alguem vetor possui apenas zeros e Verifica se o valor da normalização é razoavel\n",
    "                if (v1.sum() != 0 and v2.sum() != 0) and ((norm1 + norm2) > np.finfo(float).eps):\n",
    "                    if squared:\n",
    "                        sim_mat[i][j] = ((v1 * v2).sum() / (norm1 + norm2)) ** 2\n",
    "                    else:\n",
    "                        sim_mat[i][j] = (v1 * v2).sum() / (norm1 + norm2)\n",
    "\n",
    "        graph = nx.from_numpy_array(sim_mat)\n",
    "        pr = nx.pagerank_numpy(graph)\n",
    "\n",
    "        best[text_id] = set(sorted(pr, key=pr.get)[:n_sents])\n",
    "        \n",
    "    return best\n",
    "\n",
    "def calculate_rouge(highlights, cl_predict, pr_predict):\n",
    "    \n",
    "    rouge_results = {}\n",
    "    for text_id in cl_predict.keys():\n",
    "        this_cl_summary = \" . \".join(cl_predict[text_id])\n",
    "        this_pr_summary = \" . \".join(pr_predict[text_id])\n",
    "        this_highlights = highlights[text_id]\n",
    "        \n",
    "        this_cl_summary = re.sub(r\"[^a-z0-9\\ ]\", \"\", this_cl_summary.lower(), flags=re.DOTALL|re.MULTILINE)\n",
    "        this_pr_summary = re.sub(r\"[^a-z0-9\\ ]\", \"\", this_pr_summary.lower(), flags=re.DOTALL|re.MULTILINE)\n",
    "        this_highlights = re.sub(r\"[^a-z0-9\\ ]\", \"\", this_highlights.lower(), flags=re.DOTALL|re.MULTILINE)\n",
    "        \n",
    "        cl_score = rouge.get_scores(this_cl_summary, this_highlights)\n",
    "        pr_score = rouge.get_scores(this_pr_summary, this_highlights)\n",
    "        \n",
    "        rouge_results[text_id] = {name:result for name, result in zip([\"Cluster\", \"PageRank\"], cl_score + pr_score)}\n",
    "        \n",
    "    return rouge_results\n",
    "\n",
    "def summary_rouge(rouge_results):\n",
    "    \n",
    "    n_texts = len(rouge_results)\n",
    "\n",
    "    cl_results = {\"Rouge1\":[],\"Rouge2\":[],\"RougeL\":[]}\n",
    "    pr_results = {\"Rouge1\":[],\"Rouge2\":[],\"RougeL\":[]}\n",
    "\n",
    "    for text_id, score in rouge_results.items():\n",
    "        cl_results[\"Rouge1\"].append(np.array(list(score[\"Cluster\"]['rouge-1'].values())))\n",
    "        cl_results[\"Rouge2\"].append(np.array(list(score[\"Cluster\"]['rouge-2'].values())))\n",
    "        cl_results[\"RougeL\"].append(np.array(list(score[\"Cluster\"]['rouge-l'].values())))\n",
    "\n",
    "        pr_results[\"Rouge1\"].append(np.array(list(score[\"PageRank\"]['rouge-1'].values())))\n",
    "        pr_results[\"Rouge2\"].append(np.array(list(score[\"PageRank\"]['rouge-2'].values())))\n",
    "        pr_results[\"RougeL\"].append(np.array(list(score[\"PageRank\"]['rouge-l'].values())))\n",
    "    \n",
    "    params = [\"f\", \"p\", \"r\"]\n",
    "    params_std = [\"f_std\", \"p_std\", \"r_std\"]\n",
    "    for key in list(cl_results.keys()):\n",
    "        cl_mean, cl_std = np.mean(cl_results[key], axis=0), np.std(cl_results[key], axis=0)\n",
    "        cl_results[key] = {k:v for k, v in zip(params, cl_mean)} \n",
    "        cl_results[f\"{key}_std\"] = {k:v for k, v in zip(params_std, cl_std)}\n",
    "        \n",
    "        pr_mean, pr_std = np.mean(pr_results[key], axis=0), np.std(pr_results[key], axis=0)\n",
    "        pr_results[key] = {k:v for k, v in zip(params, pr_mean)}\n",
    "        pr_results[f\"{key}_std\"] = {k:v for k, v in zip(params_std, pr_std)}\n",
    "        \n",
    "    data = {\"Cluster\":{key:value for key, value in cl_results.items() if not \"std\" in key},\n",
    "            \"Cluster_std\":{key.replace(\"_std\", \"\"):value for key, value in cl_results.items() if \"std\" in key},  \n",
    "            \"PageRank\":{key:value for key, value in pr_results.items() if not \"std\" in key},\n",
    "            \"PageRank_std\":{key.replace(\"_std\", \"\"):value for key, value in pr_results.items() if \"std\" in key}}\n",
    "\n",
    "    for grouper in data.keys():\n",
    "        data[grouper] = {(rouge, score.replace(\"_std\", \"\")):value for rouge in data[grouper].keys() \n",
    "                                                                  for score, value in data[grouper][rouge].items()}\n",
    "                         \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def save_summary_result(model_name, summary_cl, summary_pr, rouge_results):\n",
    "    \n",
    "    if summary_cl != None:\n",
    "        with open(f\"outputs/summary_{model_name}_cl.json\", \"w\") as file:\n",
    "            json.dump(summary_cl, file)\n",
    "\n",
    "    if summary_pr != None:\n",
    "        with open(f\"outputs/summary_{model_name}_pr.json\", \"w\") as file:\n",
    "            json.dump(summary_pr, file)\n",
    "    \n",
    "    if rouge_results != None:\n",
    "        with open(f\"outputs/rouge_{model_name}.json\", \"w\") as file:\n",
    "            json.dump(rouge_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo com as sentenças\n",
    "<a id='Sent'></a>\n",
    "\n",
    "Nessa seção os modelos de embeding serão treinados no conjunto total de sentenças de todos os doumentos do corpus, em seguida serão utilizados para transformar as sentenças em vetores e esses vetores, dividos por documentos, passaram por algoritimos de clusterização e pagerank para terem suas sentenças mais relevantes selecionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "<a id='tfidf'></a>\n",
    "\n",
    "O valor tf–idf (abreviação do inglês term frequency–inverse document frequency, que significa frequência do termo–inverso da frequência nos documentos), é uma medida estatística que tem o intuito de indicar a importância de uma palavra de um documento em relação a uma coleção de documentos ou em um corpus linguístico de acordo com quantas vezes a palavra aparece nesse documento em relçaão a quantas vezes ela aparece no corpus. Nesse caso se verifia quantas palavras são importantes em cada sentença do próprio documento em relação ao documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(min_df=5, \n",
    "                        max_df=0.9, \n",
    "                        max_features=5000, \n",
    "                        sublinear_tf=False, \n",
    "                        analyzer=lambda x: x)\n",
    "\n",
    "tfidf_vecs = tfidf.fit_transform(all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cl_best = find_most_relevant_cl(tfidf_vecs, sents_reference)\n",
    "tfidf_cl_summary = {}\n",
    "for text_id in tfidf_cl_best.keys():\n",
    "    tfidf_cl_summary[text_id] = [orig_text[text_id][sent] for sent in tfidf_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pr_best = find_most_relevant_pr(tfidf_vecs, sents_reference)\n",
    "tfidf_pr_summary = {}\n",
    "for text_id in tfidf_pr_best.keys():\n",
    "    tfidf_pr_summary[text_id] = [orig_text[text_id][sent] for sent in tfidf_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faz o teste ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RougeL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.259760</td>\n",
       "      <td>0.217934</td>\n",
       "      <td>0.350955</td>\n",
       "      <td>0.077447</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.107217</td>\n",
       "      <td>0.181720</td>\n",
       "      <td>0.154687</td>\n",
       "      <td>0.235619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster_std</th>\n",
       "      <td>0.100982</td>\n",
       "      <td>0.091195</td>\n",
       "      <td>0.151006</td>\n",
       "      <td>0.079896</td>\n",
       "      <td>0.067569</td>\n",
       "      <td>0.112455</td>\n",
       "      <td>0.091264</td>\n",
       "      <td>0.079980</td>\n",
       "      <td>0.127176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.216318</td>\n",
       "      <td>0.189212</td>\n",
       "      <td>0.274902</td>\n",
       "      <td>0.049444</td>\n",
       "      <td>0.042177</td>\n",
       "      <td>0.064940</td>\n",
       "      <td>0.145535</td>\n",
       "      <td>0.128370</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank_std</th>\n",
       "      <td>0.094421</td>\n",
       "      <td>0.086779</td>\n",
       "      <td>0.135446</td>\n",
       "      <td>0.071310</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.097190</td>\n",
       "      <td>0.080917</td>\n",
       "      <td>0.072245</td>\n",
       "      <td>0.109920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rouge1                        Rouge2                      \\\n",
       "                     f         p         r         f         p         r   \n",
       "Cluster       0.259760  0.217934  0.350955  0.077447  0.063872  0.107217   \n",
       "Cluster_std   0.100982  0.091195  0.151006  0.079896  0.067569  0.112455   \n",
       "PageRank      0.216318  0.189212  0.274902  0.049444  0.042177  0.064940   \n",
       "PageRank_std  0.094421  0.086779  0.135446  0.071310  0.061034  0.097190   \n",
       "\n",
       "                RougeL                      \n",
       "                     f         p         r  \n",
       "Cluster       0.181720  0.154687  0.235619  \n",
       "Cluster_std   0.091264  0.079980  0.127176  \n",
       "PageRank      0.145535  0.128370  0.179900  \n",
       "PageRank_std  0.080917  0.072245  0.109920  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_rouge_results = calculate_rouge(highlights, tfidf_cl_summary, tfidf_pr_summary)\n",
    "tfidf_rouge_summary = summary_rouge(tfidf_rouge_results)\n",
    "tfidf_rouge_summary.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(tfidf_cl_summary.keys())[np.random.randint(0, len(tfidf_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-LRB- CNN -RRB- -- More than 4.3 million people tuned in to watch the U.S. women 's soccer team beat Japan in a 2-1 victory in the gold medal Olympic game. Shannon Boxx was just happy to be on the field. After injuring her hamstring , Boxx was sidelined for the team 's earlier game against Colombia. It was heartbreaking for the athlete to sit on the bench after all the health problems she had already battled during her journey to London. Boxx was diagnosed with lupus in 2007 when she was 30 years old. At the time she was playing for the U.S. National Team and had begun feeling extremely fatigued ; regular training sessions left her with joint pain and muscle soreness. She went public with her lupus diagnosis in April 2012 and is now working with the Lupus Foundation of America to create awareness about this chronic autoimmune disease that affects 1.5 million people in the U.S. With lupus , Boxx 's body produces auto-antibodies that attack and destroy healthy tissue because her immune system ca n't tell the difference. The auto-antibodies cause inflammation that leads to pain. Symptoms can flare up at any time , and although the disease is somewhat controlled with medication , it is a lifelong problem. We caught up with Boxx after the London Games to talk about her diagnosis and how she deals with it as an elite athlete. The following is an edited version of that interview : You have lupus , in addition to Sjogren 's Syndrome -- another autoimmune disease. How do the two conditions affect you ? Lupus is a chronic inflammatory disease that affects various parts of my body like my skin , my joints and various organs. -LRB- One symptom of lupus is the so-called `` butterfly rash , '' across a person 's cheeks and nose -- similar to the shape of a butterfly. -RRB- Sjogren 's Syndrome is an autoimmune in which your body attacks your moisture-producing glands. I deal with fatigue and joint pain just like with lupus , but it also affects my skin , my eyes and my mouth. I have to regularly use eye drops , drink lots of water and get regular checkups for both my eyes and my teeth. Keeping my inflammation down throughout my body is my biggest concern. Lupus attacking my kidneys , Nick Cannon says How has having an autoimmune disease affected your job ? As an elite athlete , it is my job to maintain a high fitness level , as well as sustain a strong mentality. Now add in a disease where my main symptoms are extreme fatigue and joint pain , and that standard becomes a little bit more difficult to maintain. I am very fortunate that I have finally found a medicine that helps control my symptoms , but a few years ago that was n't the case. I remember in 2010 going to training sessions completely exhausted and my knees throbbing from all my joint pain. I remember willing myself through those training sessions and then getting home and lying on the couch the rest of the day. Mentally , I was exhausted because I was trying to figure out the right medicines to use ; I was dealing with side effects from those medicines and I was keeping it a secret from my teammates. On the positive side , it has made me so much stronger as a person and as an athlete. I have the mentality that this disease is not going to beat me. I may have a bad day , but it wo n't stop me from trying again the next day. How do you control your symptoms ? The medicine I am on now helps control the amount of `` flare-ups '' I may have , as well as control the amount of inflammation in my body. To help with joint pain , I wear compression pants to sleep in and I have also been able to tweak our lifting routine so that I do n't have to put as much pressure on my joints. I do less Olympic lifts and more body-weight exercises. Fatigue is probably the hardest one to control because you do n't know when it 's going to hit you. The best solution for me is , if I start to feel more fatigued than normal , I will train lighter that day or even just take it off completely. Learning to live with lupus What do you eat on a regular basis ? I do n't have a specific diet. As much as I am training , my goal is to just make sure I 'm getting enough food. I do my best to eat a balanced diet , but as of right now it is n't any different than the rest of my teammates. What advice do you have for our readers who are dealing with lupus and/or Sjogrens Syndrome ? I understand that people with lupus and/or Sjogren 's range from mild to severe and no two people are alike in their struggle with either of these diseases. I believe it 's very important to have a support system -- friends , family , the Lupus foundation and the Sjogren 's foundation -- that understands what you are going through. I think it 's important that you have someone who understands that you can feel good a majority of the time but are there for you when a flare-up happens. I also believe it 's important to stay active , whatever level of activity feels comfortable to you. I hope this is where I have inspired people. I have n't let this disease stop me from doing the sport that I love. You 've just begun to talk publicly about your battles with autoimmune disease. Why did you decide to start sharing your story ? I kept my story a secret for a long time because I did n't want anyone to use it against me in my profession. As I am nearing the tail end of my career , I have realized it is a lot more important to use my voice to help bring awareness to lupus and Sjogren 's. It takes people an average of seven years to be diagnosed , and in that time period they could be doing more damage to their body. If I can help spread the word about what lupus or Sjogren 's is and what the symptoms are , maybe I can help get people in to visit a doctor sooner. Dealing with an injury during the Olympics is something every athlete fears. How do you weigh the benefits of rest/recovery over pushing through the pain to help your team ? Good question. All of these thoughts went through my head when I felt my hamstring go 15 minutes into my first game in the Olympics. In a matter of seconds , I had to weigh both options. Do I push through and maybe do more damage to my leg and maybe hurt the team in the process , 'cause I 'm not 100 % -LRB- and in the worst case scenario , be out for the rest of the tournament -RRB- : or do I take myself out and give our team the chance with a healthy body in there ? I chose the second , and even though it was hard sitting on the bench I knew I put the team first. I worked hard every day to get back as healthy as I could so I could be available if needed during the playoff rounds. It seems everything worked out in the end. My hamstring healed and I was able to start in the final game and help my team to an Olympic gold medal. Not a bad way to the end the tournament. How becoming a patient can make you stronger \""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I remember willing myself through those training sessions and then getting home and lying on the couch the rest of the day',\n",
       " 'Boxx was diagnosed with lupus in 2007 when she was 30 years old',\n",
       " 'At the time she was playing for the U.S. National Team and had begun feeling extremely fatigued ; regular training sessions left her with joint pain and muscle soreness']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I do my best to eat a balanced diet , but as of right now it is n't any different than the rest of my teammates\",\n",
       " 'I do less Olympic lifts and more body-weight exercises',\n",
       " \"-RRB- Sjogren 's Syndrome is an autoimmune in which your body attacks your moisture-producing glands\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_pr_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Cluster</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PageRank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.211</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.159</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.312</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster                 PageRank                \n",
       "  rouge-1 rouge-2 rouge-l  rouge-1 rouge-2 rouge-l\n",
       "f   0.211   0.065   0.250    0.173   0.051   0.137\n",
       "p   0.159   0.048   0.200    0.143   0.042   0.116\n",
       "r   0.312   0.097   0.333    0.219   0.065   0.167"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rouge = tfidf_rouge_results[text_id]\n",
    "\n",
    "reform = {(level1_key, level2_key): values\n",
    "          for level1_key, level2_dict in text_rouge.items()\n",
    "          for level2_key, values in level2_dict.items()}\n",
    "\n",
    "pd.DataFrame(reform).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary_result(\"tfidf\", tfidf_cl_summary, tfidf_pr_summary, tfidf_rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "<a id='cbow'></a>\n",
    "\n",
    "O CBOW consiste em utilizar o modelo Word2Vec para encontrar vetores para cada palavra do corpus de acordo com as palavras que normalmente estão em sua vizinhança. Para aplicar isso aos documentos, os vetores de cada palavra das sentenças são somados de forma que sentenças com palavras similares deveriam um vetor de soma total similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cbow = gensim.models.Word2Vec(\n",
    "    corpus_file='storage/all_sents.txt',\n",
    "    window=5,\n",
    "    size=200,\n",
    "    seed=42,\n",
    "    iter=100,\n",
    "    workers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_word_vecs(model, sent):\n",
    "    vec = np.zeros(model.wv.vector_size)\n",
    "    for word in sent:\n",
    "        if word in model:\n",
    "            vec += model.wv.get_vector(word)\n",
    "            \n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > np.finfo(float).eps:\n",
    "        vec /= norm\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_vecs = scipy.sparse.csr.csr_matrix([sum_word_vecs(cbow, sent) for sent in all_sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_cl_best = find_most_relevant_cl(cbow_vecs, sents_reference)\n",
    "cbow_cl_summary = {}\n",
    "for text_id in cbow_cl_best.keys():\n",
    "    cbow_cl_summary[text_id] = [orig_text[text_id][sent] for sent in cbow_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_pr_best = find_most_relevant_pr(cbow_vecs, sents_reference)\n",
    "cbow_pr_summary = {}\n",
    "for text_id in cbow_pr_best.keys():\n",
    "    cbow_pr_summary[text_id] = [orig_text[text_id][sent] for sent in cbow_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faz o teste ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RougeL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.257674</td>\n",
       "      <td>0.213397</td>\n",
       "      <td>0.352303</td>\n",
       "      <td>0.075178</td>\n",
       "      <td>0.061447</td>\n",
       "      <td>0.104808</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.150920</td>\n",
       "      <td>0.235734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster_std</th>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.086449</td>\n",
       "      <td>0.145930</td>\n",
       "      <td>0.078356</td>\n",
       "      <td>0.065913</td>\n",
       "      <td>0.109651</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.074910</td>\n",
       "      <td>0.121574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.218177</td>\n",
       "      <td>0.191844</td>\n",
       "      <td>0.273898</td>\n",
       "      <td>0.048831</td>\n",
       "      <td>0.042440</td>\n",
       "      <td>0.062395</td>\n",
       "      <td>0.146310</td>\n",
       "      <td>0.129873</td>\n",
       "      <td>0.178445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank_std</th>\n",
       "      <td>0.093517</td>\n",
       "      <td>0.087612</td>\n",
       "      <td>0.130342</td>\n",
       "      <td>0.069202</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.091309</td>\n",
       "      <td>0.079103</td>\n",
       "      <td>0.072535</td>\n",
       "      <td>0.103133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rouge1                        Rouge2                      \\\n",
       "                     f         p         r         f         p         r   \n",
       "Cluster       0.257674  0.213397  0.352303  0.075178  0.061447  0.104808   \n",
       "Cluster_std   0.096850  0.086449  0.145930  0.078356  0.065913  0.109651   \n",
       "PageRank      0.218177  0.191844  0.273898  0.048831  0.042440  0.062395   \n",
       "PageRank_std  0.093517  0.087612  0.130342  0.069202  0.060728  0.091309   \n",
       "\n",
       "                RougeL                      \n",
       "                     f         p         r  \n",
       "Cluster       0.179528  0.150920  0.235734  \n",
       "Cluster_std   0.086320  0.074910  0.121574  \n",
       "PageRank      0.146310  0.129873  0.178445  \n",
       "PageRank_std  0.079103  0.072535  0.103133  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_rouge_results = calculate_rouge(highlights, cbow_cl_summary, cbow_pr_summary)\n",
    "cbow_rouge_summary = summary_rouge(cbow_rouge_results)\n",
    "cbow_rouge_summary.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(cbow_cl_summary.keys())[np.random.randint(0, len(cbow_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Texas Gov. Rick Perry will immediately send up to 1,000 National Guard troops to help secure the southern border , where tens of thousands of unaccompanied minors from Central America have crossed into the United States this year in a surge that is deemed a humanitarian crisis. Perry also wants President Barack Obama and Congress to hire an additional 3,000 border patrol agents for the Texas border , which would eventually replace the temporary guard forces. `` I will not stand idly by , '' Perry said in Austin Monday , announcing what he called Operation Strong Safety. `` The price of inaction is too high. '' Perry 's state has received the majority of migrant children , especially in the Rio Grande region , and he has repeatedly called on the federal government to beef up border security. White House spokesman Josh Earnest said the White House has not yet received the formal communication required for Perry to deploy guard troops. But he said if Perry follows through , he hopes those forces would be coordinated `` with the significant ongoing efforts already in place. '' On border crisis , Jeb Bush , Rubio and Perry remake image The Rio Grande sector , where most of the immigrant children are turning themselves into the border patrol , has a large number of agents but it is also the largest crossing. It currently has 3,000 border patrol agents covering 320 miles of land and 250 miles of water , which equates to 5.4 agents per mile. The Tucson sector , for instance , has approximately 15.7 agents per mile. Both Perry and Obama have the authority to deploy National Guard troops , but whoever authorizes it has to pay for it. The deployment at Perry 's direction means Texas will have to pick up the price tag of $ 12 million per month. One border sheriff , Omar Lucio of Cameron County , said he is skeptical of Perry 's plan. `` At this time , a lot of people do things for political reasons. I do n't know that it helps , '' he told the Dallas Morning News. Conservatives largely point to border security as their top immigration priority and Perry has been an outspoken proponent of securing the border since the influx of immigrants began. Is the immigration crisis Rick Perry 's second act ? The potential 2016 presidential candidate previewed his plan in Iowa over the weekend in a television interview while campaigning for Republican candidates. He has used the issue to remake his image on immigration. His previous White House campaign was crippled partly because of a law he signed giving children of undocumented immigrants in-state college tuition. Obama asked for $ 3.7 billion in emergency funds to address the influx of young immigrants , which has n't gained much traction in Congress. It includes money to fortify the border. The House and Senate are working on their own plan , which includes a controversial proposal to change a law that prohibits the youth , mostly from El Salvador , Honduras , and Guatemala , from being immediately deported. The Obama administration questioned Perry 's motives since many of the minors are not trying to evade the border patrol but are turning themselves in after crossing the border. Children at the border : What 's happening and why But Perry said the guard will be `` force multipliers , '' helping Customs and Border Protection agents both on the ground and in the air to catch the 80 % of people crossing the border who are n't children and to combat cartel and trafficking crime. `` You can not have -LRB- national security -RRB- without border security , '' Perry said. While the number of unaccompanied youth crossing the border has doubled to nearly 60,000 in the past year , the total number of undocumented immigrants has mostly declined. About 1 million people have been caught crossing the border nearly every year between 1983 until 2006 , but that number has dropped to about 400,000 in 2013. At the same time , resources for border security have steadily increased : More than 18,000 agents patrolled the border in 2013 compared to 10,000 a decade ago. And the amount spent on border security has more than doubled. The Customs and Border Patrol budget jumped from $ 5 billion in 2002 to $ 12.4 billion this year. The issue has hit a national nerve. Protests have erupted along the border with activists demanding immediate deportation. Counter protests have also erupted , pointing out that many of the migrants are said to have fled violence at home. Protesters vent anger at ` alien invasion ' of the U.S. \""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perry also wants President Barack Obama and Congress to hire an additional 3,000 border patrol agents for the Texas border , which would eventually replace the temporary guard forces',\n",
       " 'At the same time , resources for border security have steadily increased : More than 18,000 agents patrolled the border in 2013 compared to 10,000 a decade ago',\n",
       " \"Children at the border : What 's happening and why But Perry said the guard will be `` force multipliers , '' helping Customs and Border Protection agents both on the ground and in the air to catch the 80 % of people crossing the border who are n't children and to combat cartel and trafficking crime\"]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['About 1 million people have been caught crossing the border nearly every year between 1983 until 2006 , but that number has dropped to about 400,000 in 2013',\n",
       " 'But he said if Perry follows through , he hopes those forces would be coordinated `` with the significant ongoing efforts already in place',\n",
       " \"The Obama administration questioned Perry 's motives since many of the minors are not trying to evade the border patrol but are turning themselves in after crossing the border\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_pr_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Cluster</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PageRank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.273</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster                 PageRank                \n",
       "  rouge-1 rouge-2 rouge-l  rouge-1 rouge-2 rouge-l\n",
       "f   0.273   0.053   0.190    0.254   0.032   0.078\n",
       "p   0.198   0.038   0.147    0.205   0.026   0.066\n",
       "r   0.438   0.085   0.268    0.333   0.043   0.098"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rouge = cbow_rouge_results[text_id]\n",
    "\n",
    "reform = {(level1_key, level2_key): values\n",
    "          for level1_key, level2_dict in text_rouge.items()\n",
    "          for level2_key, values in level2_dict.items()}\n",
    "\n",
    "pd.DataFrame(reform).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary_result(\"cbow\", cbow_cl_summary, cbow_pr_summary, cbow_rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "<a id='doc2vec'></a>\n",
    "\n",
    "O Doc2Vec funciona de forma similar ao Word2Vec, mas de forma a classificar documentos. Para isso além de observar as palavras do paragrafo ele utiliza o id do pargrafo para classificar que plavras são mais comuns em cada parte do documento, dessa forma, utilizando as sentenças como paragrafos o vetor gerado pelo Doc2Vec consegue descrever a sentença, utilizando, além das palavras que a compõe, o tipo da sentença também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = gensim.models.Doc2Vec(\n",
    "    corpus_file='storage/all_sents.txt',\n",
    "    vector_size=200,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=12,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_vecs = scipy.sparse.csr.csr_matrix(doc2vec.docvecs.vectors_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_cl_best = find_most_relevant_cl(doc2vec_vecs, sents_reference)\n",
    "doc2vec_cl_summary = {}\n",
    "for text_id in doc2vec_cl_best.keys():\n",
    "    doc2vec_cl_summary[text_id] = [orig_text[text_id][sent] for sent in doc2vec_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\NLP\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:335: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return dict(zip(G, map(float, largest / norm)))\n"
     ]
    }
   ],
   "source": [
    "doc2vec_pr_best = find_most_relevant_pr(doc2vec_vecs, sents_reference)\n",
    "doc2vec_pr_summary = {}\n",
    "for text_id in doc2vec_pr_best.keys():\n",
    "    doc2vec_pr_summary[text_id] = [orig_text[text_id][sent] for sent in doc2vec_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faz o teste ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RougeL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.241570</td>\n",
       "      <td>0.204844</td>\n",
       "      <td>0.320570</td>\n",
       "      <td>0.062322</td>\n",
       "      <td>0.052330</td>\n",
       "      <td>0.084075</td>\n",
       "      <td>0.166657</td>\n",
       "      <td>0.143568</td>\n",
       "      <td>0.212597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster_std</th>\n",
       "      <td>0.097813</td>\n",
       "      <td>0.089677</td>\n",
       "      <td>0.140440</td>\n",
       "      <td>0.074917</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.101684</td>\n",
       "      <td>0.086872</td>\n",
       "      <td>0.077918</td>\n",
       "      <td>0.117063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.231744</td>\n",
       "      <td>0.199216</td>\n",
       "      <td>0.299695</td>\n",
       "      <td>0.058134</td>\n",
       "      <td>0.049416</td>\n",
       "      <td>0.076256</td>\n",
       "      <td>0.157619</td>\n",
       "      <td>0.137367</td>\n",
       "      <td>0.196575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank_std</th>\n",
       "      <td>0.098235</td>\n",
       "      <td>0.090443</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.076196</td>\n",
       "      <td>0.067011</td>\n",
       "      <td>0.100473</td>\n",
       "      <td>0.086874</td>\n",
       "      <td>0.077847</td>\n",
       "      <td>0.114539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rouge1                        Rouge2                      \\\n",
       "                     f         p         r         f         p         r   \n",
       "Cluster       0.241570  0.204844  0.320570  0.062322  0.052330  0.084075   \n",
       "Cluster_std   0.097813  0.089677  0.140440  0.074917  0.065258  0.101684   \n",
       "PageRank      0.231744  0.199216  0.299695  0.058134  0.049416  0.076256   \n",
       "PageRank_std  0.098235  0.090443  0.138975  0.076196  0.067011  0.100473   \n",
       "\n",
       "                RougeL                      \n",
       "                     f         p         r  \n",
       "Cluster       0.166657  0.143568  0.212597  \n",
       "Cluster_std   0.086872  0.077918  0.117063  \n",
       "PageRank      0.157619  0.137367  0.196575  \n",
       "PageRank_std  0.086874  0.077847  0.114539  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_rouge_results = calculate_rouge(highlights, doc2vec_cl_summary, doc2vec_pr_summary)\n",
    "doc2vec_rouge_summary = summary_rouge(doc2vec_rouge_results)\n",
    "doc2vec_rouge_summary.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(doc2vec_cl_summary.keys())[np.random.randint(0, len(doc2vec_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Placerville , California -LRB- CNN -RRB- -- California 's attorney general is `` actively reviewing '' an animal charity executive who had agreed not to take a higher office with another charity after a state investigation into how her previous employer had spent its donations , a spokesman for the AG 's office told CNN. The woman at the center of the review , Terri Crisp , has been identified by SPCA International in its tax filings as one of its directors or officers. She also serves as the spokeswoman for the charity 's `` Baghdad Pups '' program which , according to SPCA International , `` helps U.S. troops safely transport home the companion animals they befriend in the war zone. '' Before her work with SPCA International , Crisp headed the California-based animal rescue charity Noah 's Wish , which received millions of dollars in donations after Hurricane Katrina struck the U.S. Gulf Coast in 2005. It promised to use the money to help animals affected by the disaster The California attorney general investigated whether contributions to Noah 's Wish for `` rescuing and caring for the animal victims of Hurricane Katrina '' were used for that purpose. In the summer of 2007 , Noah 's Wish reached a settlement agreement with the state of California in which the charity agreed to forfeit $ 4 million in donations out of the $ 8 million raised by the charity. Under that settlement , Crisp agreed not to `` serve as an officer , director or trustee or in any position having the duties or responsibilities of an officer , director or trustee , with any nonprofit organization for a period of five -LRB- 5 -RRB- years from the date of the execution of this Settlement Agreement. '' Yet in a filing with the North Carolina secretary of state 's office last year , SPCA International named Crisp in its list of officers and directors. California 's attorney general launched the investigation into Noah 's Wish after a bookkeeper with the charity and others alerted his office to questionable business practices. The former bookkeeper , who wants to conceal her identity for reasons unrelated to her work at Noah 's Wish , told CNN that donations came pouring into Noah 's Wish soon after Hurricane Katrina. Crisp had appealed for donations on numerous television networks , including CNN. `` There was cash , there were checks , there were cashiers checks , there were letters -- heartbreaking letters from kids who , instead of having birthday parties , they wanted all the money to go to Noah 's Wish to help those poor little animals , '' the woman said. `` On a given day , we would have , oh my gosh , easily $ 20,000 ... just in checks. '' And she said suddenly Terri Crisp changed , hiring her daughter and acting as if the money was hers to keep. Both made six-figure salaries , the former bookkeeper said. `` Terri at one time said , ` I 've worked hard for so many years , doing animal rescue , I am entitled to this money. ' '' When approached by CNN at her Placerville , California , home , Crisp told CNN she has `` nothing to hide '' but refused to answer any detailed questions without permission from the organization 's communications director. CNN requested an on-camera interview several weeks ago from Stephanie Scott , the SPCA International public relations director , but Scott never responded either by phone or e-mail. Standing on the lawn of her home , Crisp told CNN that `` you 've taken a lot of the information '' provided by SPCA International and `` reported it incorrectly. '' CNN said now was her chance to correct the record she saw as inaccurate. `` I would love to but as I said , I 'm an employee of SPCA International. '' She added , `` I ca n't answer any of your questions. Believe me , I would love to. '' She did tell CNN that the Noah 's Wish board of directors set her salary and that she is now an employee , not a director , at SPCA International. A CNN investigation into SPCA International found that the charity raised close to $ 27 million to help animals worldwide but spent nearly all of that money on fund-raising expenses paid to a direct-mail company. In 2010 , SPCA International owed $ 8.4 million to Quadriga Art and its affiliated company , Brickmill Marketing Services , according to publicly available Internal Revenue Service 990 tax records. Quadriga Art is one of the world 's largest direct-mail providers to charities and nonprofits. It is the same fund-raiser hired by two veterans charities that spent tens of millions of dollars for its services -- triggering a Senate investigation last month into whether one of the charities should retain its tax-exempt status. That charity , Washington-based Disabled Veterans National Foundation , collected nearly $ 56 million in donations over the past three years yet paid Quadriga Art more than $ 60 million in fees , according to a CNN investigation into the charity 's tax records. The other veterans charity , National Veterans Foundation , raised more than $ 22 million in donations over the past three years to help veterans yet spent about $ 18.2 million to pay Quadriga Art , according to IRS 990 forms. SPCA International is still in debt to Quadriga Art , according to a spokeswoman for the direct-mail firm , adding that 's part of the charity 's `` aggressive strategy '' to build a broad donor base. `` That resulted in an expected high cost in the beginning of their acquisition program , '' said the spokeswoman , who declined to be named. She called SPCA International 's efforts a `` successful strategy. '' There 's no question that a charity needs to spend money to raise money , according to Bob Ottenhoff , president of the charity watchdog group Guidestar. But he said that SPCA International 's tax records raise `` a number of red flags. '' `` No. 1 , there is an enormous amount of money going into fund-raising , '' Ottenhoff said. `` It 's not unusual for a nonprofit to fund-raise. In fact they need to fund-raise. But this organization has an enormous amount of fund-raising costs , certainly relative to the amount of money being spent. '' Of the $ 14 million raised in 2010 , SPCA International reports it spent less than 0.5 % -- about $ 60,000 -- in small cash grants to animal shelters across the United States. It also said it spent about $ 450,000 -- about 3 % of the total raised in 2010 -- to bring back animals from Iraq and Afghanistan as part of its `` Baghdad Pups '' program. In addition to its questionable finances , CNN found that SPCA International misrepresented the `` Baghdad Pups '' program on its tax filings. On its website and its tax filings , SPCA International describes as a program that `` helps U.S. troops safely transport home the companion animals they befriend in the war zone. '' Yet the charity admitted that only 26 of the nearly 500 animals transported to the United States from Iraq and Afghanistan were actually service animals. The rest were stray animals , said Stephanie Scott , the charity 's communications director. Watch Anderson Cooper 360 ° weeknights 10pm ET. For the latest from AC360 ° click here. \""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In addition to its questionable finances , CNN found that SPCA International misrepresented the `` Baghdad Pups '' program on its tax filings\",\n",
       " 'But this organization has an enormous amount of fund-raising costs , certainly relative to the amount of money being spent',\n",
       " \"'' Of the $ 14 million raised in 2010 , SPCA International reports it spent less than 0.5 % -- about $ 60,000 -- in small cash grants to animal shelters across the United States\"]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNN requested an on-camera interview several weeks ago from Stephanie Scott , the SPCA International public relations director , but Scott never responded either by phone or e-mail',\n",
       " \"Quadriga Art is one of the world 's largest direct-mail providers to charities and nonprofits\",\n",
       " \"'' Of the $ 14 million raised in 2010 , SPCA International reports it spent less than 0.5 % -- about $ 60,000 -- in small cash grants to animal shelters across the United States\"]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_pr_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" . Under a 2007 settlement , Terri Crisp agreed not to serve as a charity official . Yet , last year , she was named as one of SPCA International 's directors and officers . California 's attorney general is now reviewing Crisp 's involvement with SPCAI\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlights[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Cluster</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PageRank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.098</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster                 PageRank                \n",
       "  rouge-1 rouge-2 rouge-l  rouge-1 rouge-2 rouge-l\n",
       "f   0.074   0.019   0.090    0.145   0.037   0.083\n",
       "p   0.060   0.015   0.075    0.116   0.029   0.067\n",
       "r   0.098   0.025   0.111    0.195   0.050   0.111"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rouge = doc2vec_rouge_results[text_id]\n",
    "\n",
    "reform = {(level1_key, level2_key): values\n",
    "          for level1_key, level2_dict in text_rouge.items()\n",
    "          for level2_key, values in level2_dict.items()}\n",
    "\n",
    "pd.DataFrame(reform).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary_result(\"doc2vec\", doc2vec_cl_summary, doc2vec_pr_summary, doc2vec_rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "<a id='lda'></a>\n",
    "\n",
    "A modelagem de tópicos refere-se à tarefa de identificar os tópicos que melhor descrevem um conjunto de documentos. Esses tópicos surgirão apenas durante o processo de modelagem de tópicos (portanto chamado de latente). E uma técnica popular de modelagem de tópicos é conhecida como Alocação de Dirichlet Latente (LDA). A LDA imagina um conjunto fixo de tópicos. Cada tópico representa um conjunto de palavras. E o objetivo do LDA é mapear todos os documentos para os tópicos de modo que as palavras em cada documento sejam capturadas principalmente por esses tópicos imaginários. Assim, nesse caso, a LDA é utilizada para tentar encontrar tópicos nas sentenças dos corpus, de forma que sentenças com topicos pareidos devem ser pareidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(all_sents)\n",
    "doc2bow = [dictionary.doc2bow(sent) for sent in all_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_TOPICS = 20\n",
    "ldamodel = LdaMulticore(doc2bow, num_topics=NUM_TOPICS, id2word=dictionary, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso se queira explorar a LDA mudar para True\n",
    "if False:\n",
    "    lda_display = pyLDAvis.gensim.prepare(ldamodel, doc2bow, dictionary, sort_topics=False)\n",
    "    pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vecs = [ldamodel.get_document_topics(text) for text in doc2bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vecs = []\n",
    "for vec in raw_vecs:\n",
    "    this_vec = []\n",
    "    curr = 0\n",
    "    for i in range(NUM_TOPICS):\n",
    "        if (i == vec[curr][0]):\n",
    "            this_vec.append(vec[curr][1])\n",
    "            curr+=1\n",
    "            if curr == len(vec):\n",
    "                curr = -1\n",
    "        else:\n",
    "            this_vec.append(0)\n",
    "    lda_vecs.append(this_vec)\n",
    "    \n",
    "lda_vecs = scipy.sparse.csr.csr_matrix(lda_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cl_best = find_most_relevant_cl(lda_vecs, sents_reference)\n",
    "lda_cl_summary = {}\n",
    "for text_id in lda_cl_best.keys():\n",
    "    lda_cl_summary[text_id] = [orig_text[text_id][sent] for sent in lda_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pr_best = find_most_relevant_pr(lda_vecs, sents_reference)\n",
    "lda_pr_summary = {}\n",
    "for text_id in lda_pr_best.keys():\n",
    "    lda_pr_summary[text_id] = [orig_text[text_id][sent] for sent in lda_pr_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faz o teste ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RougeL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.244753</td>\n",
       "      <td>0.206479</td>\n",
       "      <td>0.328869</td>\n",
       "      <td>0.064587</td>\n",
       "      <td>0.053491</td>\n",
       "      <td>0.088854</td>\n",
       "      <td>0.167786</td>\n",
       "      <td>0.143892</td>\n",
       "      <td>0.215974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster_std</th>\n",
       "      <td>0.095793</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>0.144110</td>\n",
       "      <td>0.074595</td>\n",
       "      <td>0.064001</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>0.084989</td>\n",
       "      <td>0.076337</td>\n",
       "      <td>0.117416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.197228</td>\n",
       "      <td>0.293353</td>\n",
       "      <td>0.056417</td>\n",
       "      <td>0.048156</td>\n",
       "      <td>0.074433</td>\n",
       "      <td>0.154840</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>0.193202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank_std</th>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.086917</td>\n",
       "      <td>0.136365</td>\n",
       "      <td>0.072260</td>\n",
       "      <td>0.064021</td>\n",
       "      <td>0.096826</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>0.074008</td>\n",
       "      <td>0.111042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rouge1                        Rouge2                      \\\n",
       "                     f         p         r         f         p         r   \n",
       "Cluster       0.244753  0.206479  0.328869  0.064587  0.053491  0.088854   \n",
       "Cluster_std   0.095793  0.087585  0.144110  0.074595  0.064001  0.103575   \n",
       "PageRank      0.227500  0.197228  0.293353  0.056417  0.048156  0.074433   \n",
       "PageRank_std  0.093677  0.086917  0.136365  0.072260  0.064021  0.096826   \n",
       "\n",
       "                RougeL                      \n",
       "                     f         p         r  \n",
       "Cluster       0.167786  0.143892  0.215974  \n",
       "Cluster_std   0.084989  0.076337  0.117416  \n",
       "PageRank      0.154840  0.136070  0.193202  \n",
       "PageRank_std  0.081134  0.074008  0.111042  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_rouge_results = calculate_rouge(highlights, lda_cl_summary, lda_pr_summary)\n",
    "lda_rouge_summary = summary_rouge(lda_rouge_results)\n",
    "lda_rouge_summary.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(lda_cl_summary.keys())[np.random.randint(0, len(lda_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-LRB- CNN -RRB- -- Bolivian President Evo Morales on Sunday pledged to continue his hunger strike until Monday , when Congress -- including the opposition-led Senate -- is set to reconvene. Evo Morales on hunger strike at the presidential palace in Bolivia 's capital , La Paz. Morales ' speech , televised by a state-run station , was his first formal address to the nation since starting the strike Thursday in the government palace. More than three days into the strike , Morales appeared healthy during his address. The president wants the opposition-led Senate to set a date for general elections that are expected to give him another five-year term. Morales on Friday called on opposition members -- who walked out of the Congress in mid-session late Thursday -- to pass the election law , the government-run Bolivian Information Agency said. The nation 's first indigenous president reportedly carried out an 18-day hunger strike in 2002 , when he was expelled from Congress. \""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-LRB- CNN -RRB- -- Bolivian President Evo Morales on Sunday pledged to continue his hunger strike until Monday , when Congress -- including the opposition-led Senate -- is set to reconvene',\n",
       " 'More than three days into the strike , Morales appeared healthy during his address',\n",
       " \"The nation 's first indigenous president reportedly carried out an 18-day hunger strike in 2002 , when he was expelled from Congress\"]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-LRB- CNN -RRB- -- Bolivian President Evo Morales on Sunday pledged to continue his hunger strike until Monday , when Congress -- including the opposition-led Senate -- is set to reconvene',\n",
       " \"Evo Morales on hunger strike at the presidential palace in Bolivia 's capital , La Paz\",\n",
       " 'More than three days into the strike , Morales appeared healthy during his address']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_pr_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Cluster</th>\n",
       "      <th colspan=\"3\" halign=\"left\">PageRank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.476</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.391</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.610</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cluster                 PageRank                \n",
       "  rouge-1 rouge-2 rouge-l  rouge-1 rouge-2 rouge-l\n",
       "f   0.476   0.194   0.414    0.427   0.198   0.391\n",
       "p   0.391   0.159   0.367    0.355   0.164   0.347\n",
       "r   0.610   0.250   0.474    0.537   0.250   0.447"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rouge = doc2vec_rouge_results[text_id]\n",
    "\n",
    "reform = {(level1_key, level2_key): values\n",
    "          for level1_key, level2_dict in text_rouge.items()\n",
    "          for level2_key, values in level2_dict.items()}\n",
    "\n",
    "pd.DataFrame(reform).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary_result(\"lda\", lda_cl_summary, lda_pr_summary, lda_rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo com palavras chave\n",
    "<a id='Word'></a>\n",
    "\n",
    "Nessa seção os modelos de embeding serão treinados no conjunto total de palavras de todos os doumentos do corpus, em seguida serão utilizados para transformar as palavras em vetores e esses vetores, dividos por documentos, passaram por algoritimos de clusterização e pagerank para terem suas palavras mais relevantes selecionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "<a id='word2vec'></a>\n",
    "\n",
    "O modelo Word2Vec utiliza a vizinhança das palavras do documento para tentar descrever a palavra, assim palavras com vetores parecidos são encontradas, normalmente, no mesmo contexto. De forma que, para a sumarização, se duas palavras são usadas no mesmo contexto, apenas uma é necessaria para o resumir o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents, sents_reference, orig_text, highlights = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_word2vec = gensim.models.Word2Vec(\n",
    "    corpus_file='storage/all_sents.txt',\n",
    "    window=5,\n",
    "    size=200,\n",
    "    seed=42,\n",
    "    iter=100,\n",
    "    workers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "word2vec_vecs = []\n",
    "words_reference = {}\n",
    "i = 0\n",
    "for text_id, sents in sents_reference.items():\n",
    "    words_reference[text_id] = []\n",
    "    for sent in sents:\n",
    "        for word in all_sents[sent]:\n",
    "            if word in model_word2vec:\n",
    "                all_words.append(word)\n",
    "                word2vec_vecs.append(model_word2vec.wv.get_vector(word))\n",
    "                words_reference[text_id].append(i)\n",
    "                i += 1\n",
    "                \n",
    "word2vec_vecs = scipy.sparse.csr.csr_matrix(word2vec_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: No center vector found in one of the clusters\n"
     ]
    }
   ],
   "source": [
    "word2vec_cl_best = find_most_relevant_cl(word2vec_vecs, words_reference, per_cluster_sent=5)\n",
    "word2vec_cl_summary = {}\n",
    "for text_id in word2vec_cl_best.keys():\n",
    "    word2vec_cl_summary[text_id] = [all_words[word] for word in word2vec_cl_best[text_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como o Page rank trabalha com iterações de multiplicação de matrix de dimensões de NxN (N sendo o numero de vetores \n",
    "# do documento) e, quando se trata de palavras, o numero de vetores aumenta muito para documentos longos, o page rank passa\n",
    "# a demorar um tempo execivo para o calulo sendo que sua utilização não é reomendade nesse caso\n",
    "if False:\n",
    "    word2vec_pr_best = find_most_relevant_pr(word2vec_vecs, words_reference, squared=True, n_sents=15)\n",
    "    word2vec_pr_summary = {}\n",
    "    for text_id in word2vec_pr_best.keys():\n",
    "        word2vec_pr_summary[text_id] = [all_words[word] for word in word2vec_pr_best[text_id]]\n",
    "else:\n",
    "    word2vec_pr_summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olha um resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id = list(word2vec_cl_summary.keys())[np.random.randint(0, len(word2vec_cl_summary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-LRB- CNN -RRB- -- Syrian opposition forces may have executed as many as 30 people , most of them government soldiers , in rural Aleppo , according to the United Nations , which cited videos of the killings posted on the Internet in July. U.N High Commissioner for Human Rights Navi Pillay called the allegations `` deeply shocking '' and called Friday for an independent investigation into the incident , which appears to have taken place in Khan al-Assal in northern Syria. `` There needs to be a thorough independent investigation to establish whether war crimes have been committed , and those responsible for such crimes should be brought to justice , '' Pillay said in a statement. The videos , posted to the Internet between July 22 and 26 , show government soldiers being ordered to lie on the ground , bodies being collected by doctors , corpses strewn along a wall and bodies in Khan al-Assal bearing gunshot wounds to the head. Pillay 's office also has information that Syrian rebels are still holding government officers and soldiers prisoner , the statement said. Reminding opposition forces that all captured or wounded soldiers must be treated in accordance with international law , Pillay said , `` Opposition forces should not think they are immune from prosecution. They must adhere to their responsibilities under international law. '' The U.N. Independent International Commission of Inquiry on the Syrian Arab Republic has previously said both rebel groups and government forces and their affiliated militias are demonstrating `` emerging patterns of summary execution and murder. '' News of the killings came as the Local Coordination Committees , an umbrella group for the opposition , said 19 of its loyalists had been killed , including two children. Most of the deaths came in Damascus , the LCC said. In a speech to government forces Thursday -- Army Foundation Day -- President Bashar al-Assad said he was confident the government would triumph over the opposition , according to the state-run Syrian Arab News Agency. '' Had we in Syria not been confident of victory , we would not have been able to remain steadfast and resist the aggression for over two years , '' the president said , according to SANA , The U.N. human rights office also said Friday that investigators have been allowed to visit rebel-controlled Khan al-Assal as part of a separate investigation into the alleged use of chemical weapons. The rebels announced Friday they had written U.N. Secretary General Ban Ki-moon , requesting an investigation and demanding `` that any individuals found to be involved in the deployment of chemical weapons in Syria be held accountable for these crimes. '' According to the United Nations , more than 100,000 people have been killed since fighting began. Almost 2 million have fled the country , and 4 million more have been internally displaced. CNN 's Eliott C. McLaughlin contributed to this report. \""
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\". \".join(orig_text[text_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'feast',\n",
       " 'americans',\n",
       " 'pope',\n",
       " 'places',\n",
       " 'year',\n",
       " 'allen',\n",
       " 'deserves',\n",
       " 'cardinals',\n",
       " 'cape',\n",
       " 'senior',\n",
       " 'small',\n",
       " 'cardinals',\n",
       " 'catholic',\n",
       " 'jesus']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_cl_summary[text_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_pr_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary_result(\"word2vec\", word2vec_cl_summary, word2vec_pr_summary, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões\n",
    "<a id='Conc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados númericos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\"TFIDF\":tfidf_rouge_summary,\n",
    "               \"CBow\":cbow_rouge_summary,\n",
    "               \"Doc2Vec\":doc2vec_rouge_summary,\n",
    "               \"LDA\":lda_rouge_summary}\n",
    "\n",
    "all_results_mean = pd.DataFrame({(level1_key, level2_key): values\n",
    "                                  for level1_key, level2_dict in all_results.items()\n",
    "                                  for level2_key, values in level2_dict.items() if not \"std\" in level2_key}).T\n",
    "\n",
    "all_results_std = pd.DataFrame({(level1_key, level2_key.replace(\"_std\", \"\")): values\n",
    "                               for level1_key, level2_dict in all_results.items()\n",
    "                               for level2_key, values in level2_dict.items() if \"std\" in level2_key}).T\n",
    "\n",
    "order = all_results_mean.sort_values((\"Rouge1\", \"p\"), ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média do Score de todos os modelos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RougeL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.259760</td>\n",
       "      <td>0.217934</td>\n",
       "      <td>0.350955</td>\n",
       "      <td>0.077447</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.107217</td>\n",
       "      <td>0.181720</td>\n",
       "      <td>0.154687</td>\n",
       "      <td>0.235619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBow</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.257674</td>\n",
       "      <td>0.213397</td>\n",
       "      <td>0.352303</td>\n",
       "      <td>0.075178</td>\n",
       "      <td>0.061447</td>\n",
       "      <td>0.104808</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.150920</td>\n",
       "      <td>0.235734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.244753</td>\n",
       "      <td>0.206479</td>\n",
       "      <td>0.328869</td>\n",
       "      <td>0.064587</td>\n",
       "      <td>0.053491</td>\n",
       "      <td>0.088854</td>\n",
       "      <td>0.167786</td>\n",
       "      <td>0.143892</td>\n",
       "      <td>0.215974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Doc2Vec</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.241570</td>\n",
       "      <td>0.204844</td>\n",
       "      <td>0.320570</td>\n",
       "      <td>0.062322</td>\n",
       "      <td>0.052330</td>\n",
       "      <td>0.084075</td>\n",
       "      <td>0.166657</td>\n",
       "      <td>0.143568</td>\n",
       "      <td>0.212597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.231744</td>\n",
       "      <td>0.199216</td>\n",
       "      <td>0.299695</td>\n",
       "      <td>0.058134</td>\n",
       "      <td>0.049416</td>\n",
       "      <td>0.076256</td>\n",
       "      <td>0.157619</td>\n",
       "      <td>0.137367</td>\n",
       "      <td>0.196575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.197228</td>\n",
       "      <td>0.293353</td>\n",
       "      <td>0.056417</td>\n",
       "      <td>0.048156</td>\n",
       "      <td>0.074433</td>\n",
       "      <td>0.154840</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>0.193202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBow</th>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.218177</td>\n",
       "      <td>0.191844</td>\n",
       "      <td>0.273898</td>\n",
       "      <td>0.048831</td>\n",
       "      <td>0.042440</td>\n",
       "      <td>0.062395</td>\n",
       "      <td>0.146310</td>\n",
       "      <td>0.129873</td>\n",
       "      <td>0.178445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF</th>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.216318</td>\n",
       "      <td>0.189212</td>\n",
       "      <td>0.274902</td>\n",
       "      <td>0.049444</td>\n",
       "      <td>0.042177</td>\n",
       "      <td>0.064940</td>\n",
       "      <td>0.145535</td>\n",
       "      <td>0.128370</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Rouge1                        Rouge2                      \\\n",
       "                         f         p         r         f         p         r   \n",
       "TFIDF   Cluster   0.259760  0.217934  0.350955  0.077447  0.063872  0.107217   \n",
       "CBow    Cluster   0.257674  0.213397  0.352303  0.075178  0.061447  0.104808   \n",
       "LDA     Cluster   0.244753  0.206479  0.328869  0.064587  0.053491  0.088854   \n",
       "Doc2Vec Cluster   0.241570  0.204844  0.320570  0.062322  0.052330  0.084075   \n",
       "        PageRank  0.231744  0.199216  0.299695  0.058134  0.049416  0.076256   \n",
       "LDA     PageRank  0.227500  0.197228  0.293353  0.056417  0.048156  0.074433   \n",
       "CBow    PageRank  0.218177  0.191844  0.273898  0.048831  0.042440  0.062395   \n",
       "TFIDF   PageRank  0.216318  0.189212  0.274902  0.049444  0.042177  0.064940   \n",
       "\n",
       "                    RougeL                      \n",
       "                         f         p         r  \n",
       "TFIDF   Cluster   0.181720  0.154687  0.235619  \n",
       "CBow    Cluster   0.179528  0.150920  0.235734  \n",
       "LDA     Cluster   0.167786  0.143892  0.215974  \n",
       "Doc2Vec Cluster   0.166657  0.143568  0.212597  \n",
       "        PageRank  0.157619  0.137367  0.196575  \n",
       "LDA     PageRank  0.154840  0.136070  0.193202  \n",
       "CBow    PageRank  0.146310  0.129873  0.178445  \n",
       "TFIDF   PageRank  0.145535  0.128370  0.179900  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Média do Score de todos os modelos\")\n",
    "all_results_mean.loc[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desvio padrão do Score de todos os modelos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Rouge2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RougeL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.100982</td>\n",
       "      <td>0.091195</td>\n",
       "      <td>0.151006</td>\n",
       "      <td>0.079896</td>\n",
       "      <td>0.067569</td>\n",
       "      <td>0.112455</td>\n",
       "      <td>0.091264</td>\n",
       "      <td>0.079980</td>\n",
       "      <td>0.127176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBow</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.086449</td>\n",
       "      <td>0.145930</td>\n",
       "      <td>0.078356</td>\n",
       "      <td>0.065913</td>\n",
       "      <td>0.109651</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.074910</td>\n",
       "      <td>0.121574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.095793</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>0.144110</td>\n",
       "      <td>0.074595</td>\n",
       "      <td>0.064001</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>0.084989</td>\n",
       "      <td>0.076337</td>\n",
       "      <td>0.117416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Doc2Vec</th>\n",
       "      <th>Cluster</th>\n",
       "      <td>0.097813</td>\n",
       "      <td>0.089677</td>\n",
       "      <td>0.140440</td>\n",
       "      <td>0.074917</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.101684</td>\n",
       "      <td>0.086872</td>\n",
       "      <td>0.077918</td>\n",
       "      <td>0.117063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.098235</td>\n",
       "      <td>0.090443</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.076196</td>\n",
       "      <td>0.067011</td>\n",
       "      <td>0.100473</td>\n",
       "      <td>0.086874</td>\n",
       "      <td>0.077847</td>\n",
       "      <td>0.114539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.086917</td>\n",
       "      <td>0.136365</td>\n",
       "      <td>0.072260</td>\n",
       "      <td>0.064021</td>\n",
       "      <td>0.096826</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>0.074008</td>\n",
       "      <td>0.111042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBow</th>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.093517</td>\n",
       "      <td>0.087612</td>\n",
       "      <td>0.130342</td>\n",
       "      <td>0.069202</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.091309</td>\n",
       "      <td>0.079103</td>\n",
       "      <td>0.072535</td>\n",
       "      <td>0.103133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFIDF</th>\n",
       "      <th>PageRank</th>\n",
       "      <td>0.094421</td>\n",
       "      <td>0.086779</td>\n",
       "      <td>0.135446</td>\n",
       "      <td>0.071310</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.097190</td>\n",
       "      <td>0.080917</td>\n",
       "      <td>0.072245</td>\n",
       "      <td>0.109920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Rouge1                        Rouge2                      \\\n",
       "                         f         p         r         f         p         r   \n",
       "TFIDF   Cluster   0.100982  0.091195  0.151006  0.079896  0.067569  0.112455   \n",
       "CBow    Cluster   0.096850  0.086449  0.145930  0.078356  0.065913  0.109651   \n",
       "LDA     Cluster   0.095793  0.087585  0.144110  0.074595  0.064001  0.103575   \n",
       "Doc2Vec Cluster   0.097813  0.089677  0.140440  0.074917  0.065258  0.101684   \n",
       "        PageRank  0.098235  0.090443  0.138975  0.076196  0.067011  0.100473   \n",
       "LDA     PageRank  0.093677  0.086917  0.136365  0.072260  0.064021  0.096826   \n",
       "CBow    PageRank  0.093517  0.087612  0.130342  0.069202  0.060728  0.091309   \n",
       "TFIDF   PageRank  0.094421  0.086779  0.135446  0.071310  0.061034  0.097190   \n",
       "\n",
       "                    RougeL                      \n",
       "                         f         p         r  \n",
       "TFIDF   Cluster   0.091264  0.079980  0.127176  \n",
       "CBow    Cluster   0.086320  0.074910  0.121574  \n",
       "LDA     Cluster   0.084989  0.076337  0.117416  \n",
       "Doc2Vec Cluster   0.086872  0.077918  0.117063  \n",
       "        PageRank  0.086874  0.077847  0.114539  \n",
       "LDA     PageRank  0.081134  0.074008  0.111042  \n",
       "CBow    PageRank  0.079103  0.072535  0.103133  \n",
       "TFIDF   PageRank  0.080917  0.072245  0.109920  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Desvio padrão do Score de todos os modelos\")\n",
    "all_results_std.loc[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos scores rouge foi possivel observar que nenhum dos modelos obteve um sumario muito satisfatório, isso pode se dar:\n",
    "* Pela diferença de tamanho entre as sentenças selecionadas pelos algoritimos e pelo tamanho do highlight (as vezes mais palavras no highlight as vezes muitas palavras no summario) \n",
    "* O metodo de préprocessamento do texto, que ao rmover sentenças muito curtas e stopwords acabou atrapalhando o processo de rankeamento\n",
    "* A métrica rouge pode ser muito rigida para o tipo de teste realizado \n",
    "* A sumarização seja muito complexa para o nivel de desenvolvimento que foi feita para esses modelos\n",
    "\n",
    "De qualquer forma, pela validação manual foi possivel verifiar que alguns resumos ficaram razoaveis para o texto, apesar de não possuirem pontuações muito elevadas.\n",
    "\n",
    "Trabalhos Futuros: Uma boa opção para melhorar a qualidade dos sumarios poderia ser utilizar o melhor resumo criado pelos algoritimos de forma que dos 8 possiveis resumos, apenas o com maior Precisão ou F1 score fosse apresentado ao leitor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
